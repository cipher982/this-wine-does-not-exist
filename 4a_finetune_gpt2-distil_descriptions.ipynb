{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-19T20:31:03.360442Z",
     "start_time": "2020-08-19T20:31:01.794311Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add tokens from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-19T21:19:00.053020Z",
     "start_time": "2020-08-19T21:18:59.457599Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50257\n",
      "50257\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained('distilgpt2')\n",
    "print(tokenizer.vocab_size)\n",
    "print(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-19T20:31:05.287062Z",
     "start_time": "2020-08-19T20:31:05.272915Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50257\n",
      "50258\n"
     ]
    }
   ],
   "source": [
    "tokenizer.add_special_tokens(\n",
    "  {'eos_token':'<|startoftext|>',\n",
    "   'bos_token':'<|startoftext|>'\n",
    "  }\n",
    ")\n",
    "\n",
    "print(tokenizer.vocab_size)\n",
    "print(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-19T20:31:05.350061Z",
     "start_time": "2020-08-19T20:31:05.335059Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50257\n",
      "50260\n"
     ]
    }
   ],
   "source": [
    "tokenizer.add_tokens(['[prompt]','[response]'])\n",
    "\n",
    "print(tokenizer.vocab_size)\n",
    "print(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-19T03:04:31.735730Z",
     "start_time": "2020-08-19T03:04:31.645733Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('data/modeling/gpt2_distil_model/vocab.json',\n",
       " 'data/modeling/gpt2_distil_model/merges.txt',\n",
       " 'data/modeling/gpt2_distil_model/special_tokens_map.json',\n",
       " 'data/modeling/gpt2_distil_model/added_tokens.json')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained('data/modeling/gpt2_distil_model/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add GPT2 model to local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-19T20:31:09.990226Z",
     "start_time": "2020-08-19T20:31:07.641070Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\david\\documents\\github\\this-wine-does-not-exist\\transformers\\src\\transformers\\modeling_auto.py:811: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "model = transformers.AutoModelWithLMHead.from_pretrained('distilgpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-19T20:31:10.474254Z",
     "start_time": "2020-08-19T20:31:10.037230Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50260, 768)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-19T03:04:54.002332Z",
     "start_time": "2020-08-19T03:04:53.641353Z"
    }
   },
   "outputs": [],
   "source": [
    "model.save_pretrained('data/modeling/gpt2_distil_model/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the method in which Jupyter processes shell commands it won't show STDOUT live, only outputting once the run is finished. So I prefer to just paste this into a terminal instead of running in here."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-18T23:58:17.693Z"
    }
   },
   "source": [
    "!python transformers/examples/language-modeling/run_language_modeling.py \\\n",
    "--output_dir gpt2_distil_output \\\n",
    "--model_type gpt2 \\\n",
    "--model_name_or_path \"data/modeling/gpt2_distil_model/\" \\\n",
    "--do_train \\\n",
    "--train_data_file \"data/scraped/name_desc_nlp_ready_train.txt\" \\\n",
    "--do_eval \\\n",
    "--eval_data_file \"data/scraped/name_desc_nlp_ready_test.txt\" \\\n",
    "--per_gpu_train_batch_size 5"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "python transformers/examples/language-modeling/run_language_modeling.py --output_dir gpt2_distil_output --model_type gpt2 --model_name_or_path \"data/modeling/gpt2_distil_model/\" --do_train --train_data_file \"data/scraped/name_desc_nlp_ready_train.txt\" --do_eval --eval_data_file \"data/scraped/name_desc_nlp_ready_test.txt\" --per_device_train_batch_size 5 --evaluate_during_trianing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratchpad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-19T18:19:05.452801Z",
     "start_time": "2020-08-19T18:19:05.439799Z"
    }
   },
   "source": [
    "### Find unknown tokens in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-19T20:31:12.726908Z",
     "start_time": "2020-08-19T20:31:12.276909Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(125482, 6)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_csv('data/scraped/name_desc_nlp_ready.txt', sep='\\t', header=None)\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-19T20:37:51.436425Z",
     "start_time": "2020-08-19T20:37:51.429403Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "concat_row = \"    \".join(list(row.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-19T20:38:10.193797Z",
     "start_time": "2020-08-19T20:38:10.187797Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|startoftext|>    [prompt]    Caymus Napa Valley Cabernet Sauvignon (1.5 Liter Magnum) 2017    [response]    Caymus has a signature style that is dark in color, with rich fruit and ripe, velvety tannins \\x96 as approachable in youth as in maturity. We farm Cabernet grapes in eight of Napa\\x92s 16 sub-appellations, with diversification enabling us to make the best possible wine in a given year. Our Cabernet offers layered, lush aromas and flavors, including cocoa, cassis, and ripe dark berries.    <|endoftext|>'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-19T20:31:13.147001Z",
     "start_time": "2020-08-19T20:31:13.115931Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Laurent-Perrier Cuvee Rose\n",
      "[14772, 495, 429, 12, 5990, 5277, 14496, 303, 68, 8049]\n",
      "--------------------------------------------------\n",
      "Piper-Heidsieck Cuvee Brut in Travel Case with 2 Champagne Flutes\n",
      "[47, 9346, 12, 1544, 2340, 494, 694, 14496, 303, 68, 30291, 287, 13524, 8913, 351, 362, 29260, 21080, 1610, 1769]\n",
      "--------------------------------------------------\n",
      "Clarendon Hills Astralis Syrah 2011\n",
      "[2601, 533, 358, 261, 14379, 34496, 271, 1632, 11392, 2813]\n",
      "--------------------------------------------------\n",
      "Yalumba Patchwork Shiraz 2014\n",
      "[56, 282, 2178, 64, 17106, 1818, 21972, 1031, 1946]\n",
      "--------------------------------------------------\n",
      "Caymus Napa Valley Cabernet Sauvignon 2018\n",
      "[34, 323, 14664, 14332, 64, 6916, 15976, 1142, 316, 23167, 85, 570, 261, 2864]\n",
      "--------------------------------------------------\n",
      "Veuve Clicquot Yellow Label Brut\n",
      "[53, 12496, 303, 327, 677, 421, 313, 12550, 36052, 30291]\n",
      "--------------------------------------------------\n",
      "Mayacamas Cabernet Sauvignon 2015\n",
      "[6747, 330, 17485, 15976, 1142, 316, 23167, 85, 570, 261, 1853]\n",
      "--------------------------------------------------\n",
      "Roserock by Drouhin Oregon Eola-Amity Hills Pinot Noir 2016\n",
      "[49, 13416, 735, 416, 360, 472, 20079, 8819, 412, 5708, 12, 5840, 414, 14379, 13727, 313, 36961, 1584]\n",
      "--------------------------------------------------\n",
      "Chateau Cos d'Estournel 2016\n",
      "[1925, 378, 559, 10437, 288, 6, 22362, 1798, 417, 1584]\n",
      "--------------------------------------------------\n",
      "Meiomi Pinot Noir 2017\n",
      "[5308, 72, 12753, 13727, 313, 36961, 2177]\n",
      "--------------------------------------------------\n",
      "Duckhorn Napa Valley Cabernet Sauvignon 2016\n",
      "[35, 1347, 25311, 14332, 64, 6916, 15976, 1142, 316, 23167, 85, 570, 261, 1584]\n",
      "--------------------------------------------------\n",
      "Caymus Napa Valley Cabernet Sauvignon (1.5 Liter Magnum) 2017\n",
      "[34, 323, 14664, 14332, 64, 6916, 15976, 1142, 316, 23167, 85, 570, 261, 357, 16, 13, 20, 17667, 48019, 8, 2177]\n"
     ]
    }
   ],
   "source": [
    "total_tokens = 0\n",
    "total_unknown_tokens = 0\n",
    "for ix, row in dataset.iterrows():\n",
    "  print(\"-\"*50)\n",
    "  print(row[2])\n",
    "  tokenized_row = tokenizer.encode(row[2])\n",
    "  print(tokenized_row)\n",
    "  total_tokens += len(tokenized_row)\n",
    "  total_unknown_tokens += tokenized_row.count(50256)\n",
    "  \n",
    "  if ix>10:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-19T18:19:38.320331Z",
     "start_time": "2020-08-19T18:19:38.312331Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(total_unknown_tokens / total_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare GPT2 Models from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-19T20:02:34.055136Z",
     "start_time": "2020-08-19T20:02:34.041134Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 81.91M\n"
     ]
    }
   ],
   "source": [
    "gpt2_distilled = transformers.AutoModelForCausalLM.from_pretrained('distilgpt2')\n",
    "print(f\"Total parameters: {gpt2_distilled.num_parameters()/1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-19T20:02:34.009132Z",
     "start_time": "2020-08-19T20:02:33.994136Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 124.44M\n"
     ]
    }
   ],
   "source": [
    "gpt2 = transformers.AutoModelForCausalLM.from_pretrained('gpt2')\n",
    "print(f\"Total parameters: {gpt2.num_parameters()/1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-19T19:56:40.070221Z",
     "start_time": "2020-08-19T19:56:40.064220Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 354.82M\n"
     ]
    }
   ],
   "source": [
    "gpt2_medium = transformers.AutoModelForCausalLM.from_pretrained('gpt2-medium')\n",
    "print(f\"Total parameters: {gpt2_medium.num_parameters()/1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-19T20:02:34.102133Z",
     "start_time": "2020-08-19T20:02:34.088135Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 774.03M\n"
     ]
    }
   ],
   "source": [
    "gpt2_large = transformers.AutoModelForCausalLM.from_pretrained('gpt2-large')\n",
    "print(f\"Total parameters: {gpt2_large.num_parameters()/1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-19T20:02:41.110326Z",
     "start_time": "2020-08-19T20:02:41.100294Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 1557.61M\n"
     ]
    }
   ],
   "source": [
    "gpt2_xl = transformers.AutoModelForCausalLM.from_pretrained('gpt2-xl')\n",
    "print(f\"Total parameters: {gpt2_xl.num_parameters()/1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-19T20:22:13.129103Z",
     "start_time": "2020-08-19T20:22:13.111102Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.285714285714286"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1500/350"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
