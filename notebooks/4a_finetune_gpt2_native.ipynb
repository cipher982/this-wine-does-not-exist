{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset\n",
      "Encoded dataset\n",
      "Created PyTorch DataSet\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set variables\n",
    "#os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"4a_finetune_gpt2-distil_descriptions\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='1'\n",
    "\n",
    "# Setup PyTorch Dataset subclass\n",
    "class wineDataset(torch.utils.data.Dataset):\n",
    "  def __init__(self, encodings):\n",
    "    self.encodings = encodings\n",
    "            \n",
    "  def __len__(self):\n",
    "    return len(self.encodings['input_ids'])\n",
    "    \n",
    "  def __getitem__(self, idx):\n",
    "    item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "    item['labels'] = item['input_ids']\n",
    "    return item\n",
    "  \n",
    "\n",
    "# Setup tokenizer\n",
    "tokenizer = transformers.GPT2Tokenizer.from_pretrained('distilgpt2')\n",
    "tokenizer.add_special_tokens(\n",
    "  {'eos_token':'<|startoftext|>',\n",
    "   'bos_token':'<|startoftext|>'\n",
    "  }\n",
    ")\n",
    "tokenizer.add_tokens(['[prompt]','[response]','[category_1]',\n",
    "                      '[category_2]','[origin]','[description]',\n",
    "                      '<|endoftext|>'])\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "tokenizer.save_pretrained('data/modeling/gpt2_distil_model/')\n",
    "\n",
    "with open('data/scraped/name_desc_nlp_ready_train.txt', 'r', encoding='utf8') as file:\n",
    "    wines_raw_train = file.read().splitlines()\n",
    "with open('data/scraped/name_desc_nlp_ready_test.txt', 'r', encoding='utf8') as file:\n",
    "    wines_raw_test = file.read().splitlines()\n",
    "print(\"Loaded dataset\")\n",
    "\n",
    "wine_encodings_train = tokenizer(wines_raw_train, padding='max_length')\n",
    "wine_encodings_test = tokenizer(wines_raw_test, padding='max_length')\n",
    "print(\"Encoded dataset\")\n",
    "\n",
    "wine_dataset_train = wineDataset(wine_encodings_train)\n",
    "wine_dataset_test = wineDataset(wine_encodings_test)\n",
    "print(\"Created PyTorch DataSet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformers.AutoModelForCausalLM.from_pretrained('distilgpt2')\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.save_pretrained('data/modeling/gpt2_distil_model/')\n",
    "print(\"Loaded model\")\n",
    "\n",
    "training_args = transformers.TrainingArguments(\n",
    "  output_dir=\"data/gpt2_runs/distilgpt2-trainer\", #The output directory\n",
    "  overwrite_output_dir=True, #overwrite the content of the output directory\n",
    "  num_train_epochs=1, # number of training epochs\n",
    "  per_device_train_batch_size=2, # batch size for training\n",
    "  per_device_eval_batch_size=2,  # batch size for evaluation\n",
    "  eval_steps=500, # Number of update steps between two evaluations.\n",
    "  save_steps=2000, # after # steps model is saved\n",
    "  warmup_steps=500, # number of warmup steps for learning rate scheduler\n",
    ")\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "  model=model,\n",
    "  args=training_args,\n",
    "  train_dataset=wine_dataset_train,\n",
    "  eval_dataset=wine_dataset_test\n",
    ")\n",
    "print(\"Set up trainer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Native PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T16:32:06.864549Z",
     "start_time": "2020-11-18T16:32:06.352050Z"
    }
   },
   "outputs": [],
   "source": [
    "model = transformers.GPT2LMHeadModel.from_pretrained('distilgpt2')\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.train()\n",
    "\n",
    "optim = transformers.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(wine_dataset)\n",
    "\n",
    "for epoch in range(3):\n",
    "  for batch in train_loader:\n",
    "    optim.zero_grad()\n",
    "    #input_ids = batch.to(device)\n",
    "    input_ids = batch['input_ids']\n",
    "    attention_mask = batch['attention_mask']\n",
    "    print(len(input_ids))\n",
    "    #attention_mask = batch['attention_mask'].to(device)\n",
    "    #labels = batch['labels'].to(device)\n",
    "    outputs = model(**batch, labels=input_ids, return_dict=True)\n",
    "    loss = outputs['loss']\n",
    "    loss.backward()\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T03:10:05.217752Z",
     "start_time": "2020-11-18T03:10:05.212251Z"
    }
   },
   "outputs": [],
   "source": [
    "outputs.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HuggingFace Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the method in which Jupyter processes shell commands it won't show STDOUT live, only outputting once the run is finished. So I prefer to just paste this into a terminal instead of running in here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python transformers/examples/language-modeling/run_language_modeling.py \\\n",
    "--output_dir gpt2_distil_output \\\n",
    "--model_type distilgpt2 \\\n",
    "--model_name_or_path \"data/modeling/gpt2_distil_model/\" \\\n",
    "--do_train \\\n",
    "--train_data_file \"data/scraped/name_desc_nlp_ready_train.txt\" \\\n",
    "--do_eval \\\n",
    "--eval_data_file \"data/scraped/name_desc_nlp_ready_test.txt\" \\\n",
    "--per_gpu_train_batch_size 1 \\\n",
    "--overwrite_output_dir"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "329.85px",
    "left": "705.2px",
    "right": "20px",
    "top": "120px",
    "width": "299.4px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
