{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-24T21:45:41.858013Z",
     "start_time": "2021-03-24T21:45:39.027502Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.compat.v1.python_io import TFRecordWriter\n",
    "from tensorflow.train import Feature, BytesList, Int64List, Example\n",
    "from transformers import GPT2TokenizerFast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset and tokenize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-24T21:45:57.507948Z",
     "start_time": "2021-03-24T21:45:41.892014Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded wine dataset of length: 246,584\n",
      "Cleaned dataset has 71,634 samples\n",
      "Loaded tokenizer\n",
      "Modified tokenizer tokens\n",
      "Saved tokenizer to ./tokenizer_gpt2\n",
      "Encoded dataset\n"
     ]
    }
   ],
   "source": [
    "# Load wine dataset\n",
    "wines_path = \"C:/Users/david/Documents/github/this-wine-does-not-exist/data/scraped/name_desc_nlp_ready.txt\"\n",
    "with open(wines_path, 'r', encoding='utf8') as f:\n",
    "    wines_raw = f.read().splitlines()\n",
    "print(f\"Loaded wine dataset of length: {len(wines_raw):,}\")\n",
    "\n",
    "# Remove wines with too short descriptions\n",
    "wines_clean = []\n",
    "for i in wines_raw:\n",
    "    try:\n",
    "        desc = i.split(\"[description]\")[1]\n",
    "        if len(desc) > 150:\n",
    "            wines_clean.append(i)\n",
    "    except:\n",
    "        pass\n",
    "print(f\"Cleaned dataset has {len(wines_clean):,} samples\")\n",
    "\n",
    "tokenizer = transformers.GPT2TokenizerFast.from_pretrained('gpt2')\n",
    "print(\"Loaded tokenizer\")\n",
    "\n",
    "\n",
    "tokenizer.add_special_tokens(\n",
    "    {'eos_token':'<|startoftext|>',\n",
    "     'bos_token':'<|startoftext|>'\n",
    "    }\n",
    ")\n",
    "tokenizer.add_tokens(['[prompt]','[response]','[category_1]',\n",
    "                      '[category_2]','[origin]','[description]',\n",
    "                      '<|endoftext|>'])\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "print(\"Modified tokenizer tokens\")\n",
    "#tokenizer_path = f'./tokenizer_gpt2'\n",
    "#tokenizer.save_pretrained(tokenizer_path)\n",
    "#print(f\"Saved tokenizer to {tokenizer_path}\")\n",
    "\n",
    "wine_encodings = tokenizer(wines_clean, max_length=250, padding=True, truncation=True)\n",
    "print(f\"Encoded dataset with attributes: {wine_encodings.keys()}\")\n",
    "print(f\"Total encoded samples: {len(wine_encodings['input_ids']):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-25T19:18:20.361488Z",
     "start_time": "2021-03-25T19:18:20.354487Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serialize to TFRecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-24T22:50:19.484624Z",
     "start_time": "2021-03-24T22:50:12.216628Z"
    }
   },
   "outputs": [],
   "source": [
    "tfrecord_file_name = \"scraped_wines_tfr\"\n",
    "with tf.compat.v1.python_io.TFRecordWriter(tfrecord_file_name) as writer:\n",
    "  for ix, wine_desc in enumerate(wines_clean):\n",
    "    features = tf.train.Features(\n",
    "      feature = {\n",
    "        'text': tf.train.Feature(\n",
    "          bytes_list = tf.train.BytesList(value = [bytes(wine_desc, 'utf-8')])),\n",
    "        'input_ids': tf.train.Feature(\n",
    "          int64_list = tf.train.Int64List(value = wine_encodings['input_ids'][ix])),\n",
    "        'attention_mask': tf.train.Feature(\n",
    "          int64_list = tf.train.Int64List(value = wine_encodings['attention_mask'][ix]))\n",
    "      }\n",
    "    )\n",
    "    example = tf.train.Example(features=features)\n",
    "    writer.write(example.SerializeToString())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
