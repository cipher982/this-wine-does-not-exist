{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "from requests import get\n",
    "import multiprocessing\n",
    "import lxml\n",
    "import csv\n",
    "\n",
    "from scraperFuncs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_wine_list: (130348, 5)\n"
     ]
    }
   ],
   "source": [
    "winesRaw = pd.read_pickle('data/scraped/scrape.pickle')\n",
    "winesRaw = winesRaw.reset_index(drop=True)\n",
    "print(f\"df_wine_list: {winesRaw.shape}\")\n",
    "\n",
    "# Read in the downloaded filenames\n",
    "#wine_pages = glob.glob('data/wine_pages/wine_*', recursive=True)\n",
    "#wine_pages = np.sort(wine_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>image_path</th>\n",
       "      <th>price</th>\n",
       "      <th>url</th>\n",
       "      <th>raw_html</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dom Perignon Lenny Kravitz Limited Edition wit...</td>\n",
       "      <td>/product/images/fl_progressive/lmgmud1xsenlouw...</td>\n",
       "      <td>199.97</td>\n",
       "      <td>/product/Dom-Perignon-Lenny-Kravitz-Limited-Ed...</td>\n",
       "      <td>&lt;div class=\"prodItem_wrap\"&gt;\\n&lt;div class=\"prodI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Louis Roederer Cristal Brut with Two Flutes an...</td>\n",
       "      <td>/product/images/fl_progressive/iqec7e0nf6mzyof...</td>\n",
       "      <td>329.99</td>\n",
       "      <td>/product/Louis-Roederer-Cristal-Brut-with-Two-...</td>\n",
       "      <td>&lt;div class=\"prodItem_wrap\"&gt;\\n&lt;div class=\"prodI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Laurent-Perrier Cuvee Rose</td>\n",
       "      <td>/product/images/fl_progressive/10521.jpg</td>\n",
       "      <td>79.99</td>\n",
       "      <td>/product/Laurent-Perrier-Cuvee-Rose/10521</td>\n",
       "      <td>&lt;div class=\"prodItem_wrap\"&gt;\\n&lt;div class=\"prodI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Piper-Heidsieck Cuvee Brut in Travel Case with...</td>\n",
       "      <td>/product/images/fl_progressive/aqi87aqobd3zc56...</td>\n",
       "      <td>79.99</td>\n",
       "      <td>/product/Piper-Heidsieck-Cuvee-Brut-in-Travel-...</td>\n",
       "      <td>&lt;div class=\"prodItem_wrap\"&gt;\\n&lt;div class=\"prodI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Clarendon Hills Astralis Syrah 2011</td>\n",
       "      <td>/product/images/fl_progressive/qajrqr4d6ttn2pf...</td>\n",
       "      <td>149.99</td>\n",
       "      <td>/product/Clarendon-Hills-Astralis-Syrah-2011/5...</td>\n",
       "      <td>&lt;div class=\"prodItem_wrap\"&gt;\\n&lt;div class=\"prodI...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                name  \\\n",
       "0  Dom Perignon Lenny Kravitz Limited Edition wit...   \n",
       "1  Louis Roederer Cristal Brut with Two Flutes an...   \n",
       "2                         Laurent-Perrier Cuvee Rose   \n",
       "3  Piper-Heidsieck Cuvee Brut in Travel Case with...   \n",
       "4                Clarendon Hills Astralis Syrah 2011   \n",
       "\n",
       "                                          image_path   price  \\\n",
       "0  /product/images/fl_progressive/lmgmud1xsenlouw...  199.97   \n",
       "1  /product/images/fl_progressive/iqec7e0nf6mzyof...  329.99   \n",
       "2           /product/images/fl_progressive/10521.jpg   79.99   \n",
       "3  /product/images/fl_progressive/aqi87aqobd3zc56...   79.99   \n",
       "4  /product/images/fl_progressive/qajrqr4d6ttn2pf...  149.99   \n",
       "\n",
       "                                                 url  \\\n",
       "0  /product/Dom-Perignon-Lenny-Kravitz-Limited-Ed...   \n",
       "1  /product/Louis-Roederer-Cristal-Brut-with-Two-...   \n",
       "2          /product/Laurent-Perrier-Cuvee-Rose/10521   \n",
       "3  /product/Piper-Heidsieck-Cuvee-Brut-in-Travel-...   \n",
       "4  /product/Clarendon-Hills-Astralis-Syrah-2011/5...   \n",
       "\n",
       "                                            raw_html  \n",
       "0  <div class=\"prodItem_wrap\">\\n<div class=\"prodI...  \n",
       "1  <div class=\"prodItem_wrap\">\\n<div class=\"prodI...  \n",
       "2  <div class=\"prodItem_wrap\">\\n<div class=\"prodI...  \n",
       "3  <div class=\"prodItem_wrap\">\\n<div class=\"prodI...  \n",
       "4  <div class=\"prodItem_wrap\">\\n<div class=\"prodI...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "winesRaw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "wines = winesRaw.copy()\n",
    "\n",
    "wines.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape and Extract Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "all_urls = winesRaw['url']\n",
    "all_urls = all_urls[:40]\n",
    "\n",
    "p = Pool(10)\n",
    "url_list, desc_list = zip(*p.map(scrapeAndExtract, all_urls))\n",
    "p.terminate()\n",
    "p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "all_urls = winesRaw['url']\n",
    "#all_urls = all_urls[:10]\n",
    "#all_urls = [\"/product/Crossbarn-by-Paul-Hobbs-Napa-Valley-Cabernet-Sauvignon-375ml-half-bottle-2016/518519\"]\n",
    "\n",
    "\n",
    "def mp_handler():\n",
    "    #p = multiprocessing.Pool(20)\n",
    "    with multiprocessing.Pool(10) as p:\n",
    "        with open('data/scraped/descriptions.csv', 'a') as f:\n",
    "            writer= csv.writer(f, lineterminator = '\\n', \n",
    "                               delimiter=\"|\", \n",
    "                               quoting=csv.QUOTE_NONNUMERIC)\n",
    "            quoting=csv.QUOTE_NONNUMERIC\n",
    "            for result in p.imap(scrapeAndExtract, all_urls):\n",
    "                print(result[0])\n",
    "                #print(result[1][:5])\n",
    "                try: \n",
    "                    desc = result[1].strip('\\\"')\n",
    "                except Exception as e:\n",
    "                    desc = str(e)\n",
    "                writer.writerow([result[0],desc])\n",
    "                # (filename, count) tuples from worker\n",
    "                #f.write(f\"URL:{result[0]}|DESC:{result[1]}\")\n",
    "            \n",
    "            \n",
    "mp_handler()\n",
    "\n",
    "p.terminate()\n",
    "p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "all_urls = winesRaw['url']\n",
    "#all_urls = all_urls[:40]\n",
    "\n",
    "p = Pool(10)\n",
    "url_list, rawHTML_list = zip(*p.map(scrapeHTML, all_urls))\n",
    "p.terminate()\n",
    "p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "p = Pool(16)\n",
    "url_list, rawHTML_list = zip(*p.map(extractDescription, rawHTML_list))\n",
    "p.terminate()\n",
    "p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descLengths = [len(desc[1]) for desc in descriptions]\n",
    "descLengths = pd.Series(descLengths)\n",
    "descLengths = descLengths[descLengths < 100]\n",
    "print(len(descLengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"/product/dom-perignon-lenny-kravitz-limited-edition-with-gift-box-2008/544740\"\n",
    "html = get(\"http://www.wine.com\"+url).text\n",
    "\n",
    "soup = BeautifulSoup(html, \"lxml\")\n",
    "#allText = soup.prettify()\n",
    "#soup2 = BeautifulSoup(allText, \"lxml\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find(\"div\", {\"itemprop\":\"description\"})\\\n",
    "    .find(\"div\", {\"class\":\"viewMoreModule_text\"}).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = winesRaw['raw_html'][0]\n",
    "soup = BeautifulSoup(html, \"lxml\")\n",
    "soup.find(\"div\", {\"itemprop\":\"description\"})\\\n",
    "    .find(\"div\", {\"class\":\"viewMoreModule_text\"}).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find(\"div\",{\"itemprop\":\"description\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'<div class=\"viewMoreModule_text\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraper(url):\n",
    "    print(url)\n",
    "    try:\n",
    "        html = get(\"http://www.wine.com\"+url).text\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        allText = soup.prettify()\n",
    "    except Exception as e:\n",
    "        return (url, e)\n",
    "    \n",
    "    try:\n",
    "        desc = allText.split(\"pipWineNotes_copy\")[1]\\\n",
    "                      .split(\"viewMoreModule_text\")[1]\\\n",
    "                      .split(\">\\n         <p>\\n          \")[1]\\\n",
    "                      .split(\"\\n         </p\")[0]\n",
    "    except IndexError:\n",
    "        allText.split(\"pipWineNotes_copy\")[1]\\\n",
    "               .split(\"viewMoreModule_text\")[1]\\\n",
    "               .split(\"</div>\")[0]\\\n",
    "        \n",
    "        return allText\n",
    "    except Exception as e:\n",
    "        desc = str(e)\n",
    "        \n",
    "    return (url, desc)\n",
    "\n",
    "page = \"Veuve-Clicquot-Yellow-Label-Brut/528\"\n",
    "allText = scraper(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allText.split(\"pipWineNotes_copy\")[1]\\\n",
    "                      .split(\"viewMoreModule_text\")[1]\\\n",
    "                      .split(\"</div>\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from queue import Queue, Empty\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "\n",
    "class MultiThreadScraper:\n",
    "\n",
    "    def __init__(self, base_url):\n",
    "\n",
    "        self.base_url = base_url\n",
    "        self.root_url = '{}://{}'.format(urlparse(self.base_url).scheme, urlparse(self.base_url).netloc)\n",
    "        self.pool = ThreadPoolExecutor(max_workers=20)\n",
    "        self.scraped_pages = set([])\n",
    "        self.to_crawl = Queue()\n",
    "        self.to_crawl.put(self.base_url)\n",
    "\n",
    "    def parse_links(self, html):\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        links = soup.find_all('a', href=True)\n",
    "        for link in links:\n",
    "            url = link['href']\n",
    "            if url.startswith('/') or url.startswith(self.root_url):\n",
    "                url = urljoin(self.root_url, url)\n",
    "                if url not in self.scraped_pages:\n",
    "                    self.to_crawl.put(url)\n",
    "\n",
    "    def scrape_info(self, html):\n",
    "        return\n",
    "\n",
    "    def post_scrape_callback(self, res):\n",
    "        result = res.result()\n",
    "        if result and result.status_code == 200:\n",
    "            self.parse_links(result.text)\n",
    "            self.scrape_info(result.text)\n",
    "\n",
    "    def scrape_page(self, url):\n",
    "        try:\n",
    "            res = requests.get(url, timeout=(3, 30))\n",
    "            return res\n",
    "        except requests.RequestException:\n",
    "            return\n",
    "\n",
    "    def run_scraper(self):\n",
    "        while True:\n",
    "            try:\n",
    "                target_url = self.to_crawl.get(timeout=60)\n",
    "                if target_url not in self.scraped_pages:\n",
    "                    print(\"Scraping URL: {}\".format(target_url))\n",
    "                    self.scraped_pages.add(target_url)\n",
    "                    job = self.pool.submit(self.scrape_page, target_url)\n",
    "                    job.add_done_callback(self.post_scrape_callback)\n",
    "            except Empty:\n",
    "                return\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                continue\n",
    "if __name__ == '__main__':\n",
    "    s = MultiThreadScraper(\"http://www.example.co.uk\")\n",
    "    s.run_scraper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "winesClean = winesRaw.copy()\n",
    "\n",
    "# Clean in duplicate indices\n",
    "winesClean.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# Scrape descriptions from site\n",
    "ix = []\n",
    "name = []\n",
    "descList = []\n",
    "for i in enumerate(tqdm(winesRaw['url'])):\n",
    "    #print(i[0])\n",
    "    try:\n",
    "        html = get(\"http://www.wine.com\"+i[1]).text\n",
    "        soup = BeautifulSoup(html)\n",
    "        allText = soup.prettify()\n",
    "        desc = allText.split(\"pipWineNotes_copy\")[1]\\\n",
    "                      .split(\"viewMoreModule_text\")[1]\\\n",
    "                      .split(\">\\n         <p>\\n          \")[1]\\\n",
    "                      .split(\"\\n         </p\")[0]\n",
    "    except IndexError:\n",
    "        #print(\"oops\")\n",
    "        desc = \"IndexError\"\n",
    "        \n",
    "    \n",
    "    ix.append()\n",
    "    name.append(winesRaw['name']i[0])\n",
    "    descList.append(desc)\n",
    "    \n",
    "    if i[0] % 1000 == 0:\n",
    "        pd.Series(descList).to_csv(f\"data/scraped/descriptions/descriptionsUpTo{i[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(i[1])\n",
    "allText = soup.prettify()\n",
    "\n",
    "allText.split(\"pipWineNotes_copy\")[1]\\\n",
    "                     # .split(\"viewMoreModule_text\")[1]\\\n",
    "                     # .split(\">\\n         <p>\\n          \")[1]\\\n",
    "                     # .split(\"\\n         </p\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winesClean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in enumerate(tqdm(list([1,2,3,4]))):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in enumerate(tqm(winesRaw['raw_html'])):\n",
    "    #print(html)\n",
    "    try:\n",
    "        desc = html.split('<div class=\"viewMoreModule_text\"><p>')[1].split('<')[0]\n",
    "    except IndexError:\n",
    "        desc = html.split('<div class=\"viewMoreModule_text\">')[1].split('<')[0]\n",
    "    \n",
    "    print(desc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read, decode, and save wine descriptions & names\n",
    "names = []\n",
    "descriptions = []\n",
    "for page in tqdm(wine_pages):\n",
    "    #print(page)\n",
    "    with open(page, \"rb\") as f:\n",
    "        #wine = BeautifulSoup(f.read(), parse_only=)\n",
    "        wine = f.read().decode('utf-8')\n",
    "    try:  \n",
    "        names.append(wine.split('><title>')[1].split('|')[0])\n",
    "    except IndexError:\n",
    "        continue\n",
    "    try:\n",
    "        descriptions.append(wine.split('<div class=\"viewMoreModule_text\"><p>')[1].split('<')[0])\n",
    "    except IndexError:\n",
    "        descriptions.append(wine.split('<div class=\"viewMoreModule_text\">')[1].split('<')[0])\n",
    "        \n",
    "pd.to_pickle((names,descriptions), path='data/scraped/cleaned_data.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Name, Description, and Price information from data\n",
    "# Some of this is redundant, but it works \n",
    "\n",
    "OUTPUT_PATH = 'data/scraped/names_prices_descriptions.pickle'\n",
    "descriptions = pd.read_pickle('data/scraped/descriptions.pickle')\n",
    "\n",
    "counter = 0\n",
    "list1 = []\n",
    "list2 = []\n",
    "list3 = []\n",
    "for name in tqdm(descriptions[0]):\n",
    "    price = df_scraped[df_scraped['name'] == name.strip()]['price'].values\n",
    "    desc = descriptions[1][counter]\n",
    "    list1.append(name.strip())\n",
    "    list2.append(price)\n",
    "    list3.append(desc)\n",
    "    counter += 1\n",
    "    \n",
    "pd.DataFrame({'name'        : list1,\n",
    "              'price'       : list2,\n",
    "              'description' : list3})\\\n",
    "    .to_pickle(OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
