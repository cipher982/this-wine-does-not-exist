seed: 99

data:
  dataset_path: data/processed/wine_training_dataset_v1.parquet
  val_fraction: 0.05
  shuffle_seed: 99
  max_samples: null
  max_eval_samples: null

model:
  base_model: gpt2
  tokenizer: gpt2
  max_length: 384
  use_lora: false  # Full fine-tuning for better quality

optimizer:
  learning_rate: 3.0e-05
  weight_decay: 0.01

trainer:
  output_dir: artifacts/full_wine_model
  epochs: 3
  per_device_train_batch_size: 16  # Optimized for M3 Pro Max unified memory
  per_device_eval_batch_size: 16
  gradient_accumulation_steps: 2   # Effective batch size: 32
  warmup_steps: 500
  eval_strategy: steps
  eval_steps: 250
  save_strategy: steps
  save_steps: 500
  logging_steps: 50
  max_steps: null
  bf16: false  # MPS doesn't support bf16 yet
  fp16: false  # MPS not recognized as GPU by Accelerate
  dataloader_num_workers: 0  # MPS works better with 0 workers
  dataloader_pin_memory: false

logging:
  use_wandb: true
  project: wine-modernization
  entity: null
  run_name: full-wine-training-gpt2
  sample_preview_interval: 200  # Generate samples every 200 steps
