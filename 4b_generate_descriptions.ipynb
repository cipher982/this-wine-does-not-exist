{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:42:44.546367Z",
     "start_time": "2021-01-27T16:42:44.541367Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7.0\n",
      "4.2.1\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict \n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import torch\n",
    "import tqdm\n",
    "import transformers\n",
    "\n",
    "print(torch.__version__)\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load tokenizer and fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:38:28.722673Z",
     "start_time": "2021-01-27T16:38:10.751263Z"
    }
   },
   "outputs": [],
   "source": [
    "TOKENIZER_PATH = \"data/gpt2_runs/tokenizers/gpt2_large\"\n",
    "MODEL_PATH = \"data/gpt2_runs/v2/step_290000/\"\n",
    "\n",
    "tokenizer = transformers.GPT2Tokenizer.from_pretrained(TOKENIZER_PATH)\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "  MODEL_PATH,\n",
    "  pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "model = model.to('cuda:1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Greedy Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:38:36.077403Z",
     "start_time": "2021-01-27T16:38:28.738674Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens:  tensor([[50257, 50258, 21902,   353,  9530,   741, 14496,   303,    68, 47535,\n",
      "           344,   609, 19917,    77,   323,  1853, 50259]], device='cuda:1')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\miniconda3\\envs\\.conda_w10\\lib\\site-packages\\torch\\utils\\checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|startoftext|> [prompt] Walter Hansel Cuvee Alyce Chardonnay 2015 [response] [category_1] \"White Wine\" [category_2] Chardonnay [origin] \" from Russian River, Sonoma County, California\" [description] \"This wine is a blend of fruit from the estate vineyard in the Russian River Valley. The majority of the blend is comprised of fruit from the younger vines in the Russian River Valley, and the balance is comprised of fruit from the warmer southern end of the Russian River Valley. The wine is barrel fermented, and malolactic fermentation occurs in 30% to 70% of the blend. The wine was aged in 30% French and 70% American oak barrels for 10 months. The wine was bottled unfined and unfiltered after 22 months barrel aging.\" <|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "input_prompt = \"<|startoftext|> [prompt] Walter Hansel Cuvee Alyce Chardonnay 2015 [response] \"\n",
    "input_ids = tokenizer.encode(input_prompt, return_tensors='pt').to('cuda:1')\n",
    "print(\"Tokens: \", input_ids)\n",
    "\n",
    "greedy_output = model.generate(input_ids, max_length=200)\n",
    "print(tokenizer.decode(greedy_output[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beam Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:38:40.453403Z",
     "start_time": "2021-01-27T16:38:36.094402Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "--------------------------------------------------------------------------------\n",
      "====================\n",
      "0: [prompt] Walter Hansel Cuvee Alyce Chardonnay 2015 [response] [category_1] \"White Wine\" [category_2] \"Other White Blends\" [origin] \" from Russian River, Sonoma County, California\" [description]\n",
      "\n",
      "\n",
      "====================\n",
      "1: [prompt] Walter Hansel Cuvee Alyce Chardonnay 2015 [response] [category_1] \"Red Wine\" [category_2] \"Other White Blends\" [origin] \" from Russian River, Sonoma County, California\" [description]\n",
      "\n",
      "\n",
      "====================\n",
      "2: [prompt] Walter Hansel Cuvee Alyce Chardonnay 2015 [response] [category_1] \"Red Wine\" [category_2] \"Other Red Blends\" [origin] \" from Russian River, Sonoma County, California\" [description]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_prompt = \"<|startoftext|> [prompt] Walter Hansel Cuvee Alyce Chardonnay 2015 [response] \"\n",
    "input_ids = tokenizer.encode(input_prompt, return_tensors='pt').to('cuda:1')\n",
    "\n",
    "# activate beam search and early_stopping\n",
    "beam_outputs = model.generate(\n",
    "  input_ids, \n",
    "  max_length=200, \n",
    "  num_beams=5, \n",
    "  no_repeat_ngram_size=2,\n",
    "  num_return_sequences=3, \n",
    "  early_stopping=True\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 80 * '-')\n",
    "for i, beam_output in enumerate(beam_outputs):\n",
    "  print(\"=\"*20)\n",
    "  print(\"{}: {}\".format(i, tokenizer.decode(beam_output, skip_special_tokens=True)))\n",
    "  print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Sampling Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:38:47.745403Z",
     "start_time": "2021-01-27T16:38:40.485403Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[50257, 50258,    53,   437,   858,   609, 19917,    77,   323, 50259]])\n",
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "====================\n",
      "[prompt] Vendange Chardonnay [response] [category_1] \"White Wine\" [category_2] Chardonnay [origin] \" from Margaret River, Western Australia, Australia\" [description] \"The fruit was sourced from both estate-owned and contracted vineyards in the Margaret River region. The majority of the blend was fermented in stainless steel tanks, while the balance was aged in French and American oak barrels. The wine has a pale straw colour with green hues. Aromas of fresh cut pear, white nectarine and a hint of lime blossom are followed by a palate showing intense peach and grapefruit characters with a hint of spice and a soft, creamy texture. A wine of weight, balance and complexity that will develop further complexity with careful cellaring.\" \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_prompt = \"<|startoftext|> \" + \"[prompt] \" + \"Vendange Chardonnay \" + \"[response] \"\n",
    "input_ids = tokenizer.encode(input_prompt, return_tensors='pt')\n",
    "print(input_ids)\n",
    "\n",
    "# Send to GPU\n",
    "model.to('cuda:1')\n",
    "input_ids = input_ids.to('cuda:1')\n",
    "\n",
    "sample_output = model.generate(\n",
    "    input_ids, \n",
    "    do_sample=True, \n",
    "    max_length=250, \n",
    "    top_p=0.8,\n",
    "    top_k=200,\n",
    "    temperature=0.9,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    bos_token_id=tokenizer.eos_token_id,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(\"=\"*20)\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate descriptions on fake wine names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:42:52.318899Z",
     "start_time": "2021-01-27T16:42:52.306900Z"
    }
   },
   "outputs": [],
   "source": [
    "names_path = 'data/fake/fake_names_12184_2020-11-19.pickle'\n",
    "with open(names_path, 'rb',) as file:\n",
    "    fake_names = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:42:54.340038Z",
     "start_time": "2021-01-27T16:42:54.324038Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'head'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-aa272c305f81>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfake_names\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'head'"
     ]
    }
   ],
   "source": [
    "fake_names.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T18:09:15.079845Z",
     "start_time": "2021-01-27T16:46:19.347853Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1000 [00:00<?, ?it/s]C:\\Users\\david\\miniconda3\\envs\\.conda_w10\\lib\\site-packages\\torch\\utils\\checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 1000/1000 [1:22:55<00:00,  4.98s/it]\n"
     ]
    }
   ],
   "source": [
    "# Send to GPU\n",
    "model.to('cuda:1')\n",
    "#input_ids = input_ids.to('cuda:1')\n",
    "\n",
    "generated_descriptions = OrderedDict()\n",
    "for fake_name in tqdm.tqdm(fake_names[:1000]):\n",
    "  #print(f\"Name: {fake_name}\")\n",
    "  \n",
    "  # Create token from fake wine name\n",
    "  try:\n",
    "    input_ids = tokenizer.encode(\n",
    "      text=(\"<|startoftext|> [prompt] \" + fake_name + \" \" + \"[response] \"), \n",
    "      return_tensors='pt'\n",
    "    ).to('cuda:1')\n",
    "  \n",
    "    # Generate a fake description based on the name\n",
    "    model_output = model.generate(\n",
    "      input_ids, \n",
    "      do_sample=True, \n",
    "      max_length=300,\n",
    "      min_length=80,\n",
    "      top_p=0.8,\n",
    "      top_k=200,\n",
    "      temperature=0.9,\n",
    "      eos_token_id=tokenizer.eos_token_id,\n",
    "      bos_token_id=tokenizer.bos_token_id,\n",
    "      early_stopping=True\n",
    "    )\n",
    "\n",
    "    generated_descriptions[fake_name] = tokenizer.decode(\n",
    "      token_ids=model_output[0], \n",
    "      skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "  except:\n",
    "    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T01:28:57.065436Z",
     "start_time": "2021-01-28T01:28:57.023438Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n",
      "(1000, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>category_1</th>\n",
       "      <th>category_2</th>\n",
       "      <th>origin</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Piul Bothen Cabernet Sauvignon 2014</td>\n",
       "      <td>Red Wine</td>\n",
       "      <td>Cabernet Sauvignon</td>\n",
       "      <td>from North Coast, California</td>\n",
       "      <td>The color of this wine is deep purple with ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Seacuscini Resantzass 2015</td>\n",
       "      <td>Red Wine</td>\n",
       "      <td>Other Red Blends</td>\n",
       "      <td>from Tuscany, Italy</td>\n",
       "      <td>The perfect wine for the holidays, this wine ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alpanena Vriestioge 2016</td>\n",
       "      <td>Red Wine</td>\n",
       "      <td>Tempranillo</td>\n",
       "      <td>from Spain</td>\n",
       "      <td>The wine is very approachable and fruit-forwa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sanmedarbecer Edena Cabernet</td>\n",
       "      <td>Red Wine</td>\n",
       "      <td>Cabernet Sauvignon</td>\n",
       "      <td>from Spain</td>\n",
       "      <td>The grapes for this wine come from a selectio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sauvignon 2001</td>\n",
       "      <td>White Wine</td>\n",
       "      <td>Sauvignon Blanc</td>\n",
       "      <td>from North Coast, California</td>\n",
       "      <td>Sauvignon Blanc grapes from the North Coast v...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  name    category_1            category_2  \\\n",
       "0  Piul Bothen Cabernet Sauvignon 2014     Red Wine    Cabernet Sauvignon    \n",
       "1           Seacuscini Resantzass 2015     Red Wine      Other Red Blends    \n",
       "2             Alpanena Vriestioge 2016     Red Wine           Tempranillo    \n",
       "3         Sanmedarbecer Edena Cabernet     Red Wine    Cabernet Sauvignon    \n",
       "4                       Sauvignon 2001   White Wine       Sauvignon Blanc    \n",
       "\n",
       "                            origin  \\\n",
       "0    from North Coast, California    \n",
       "1             from Tuscany, Italy    \n",
       "2                      from Spain    \n",
       "3                      from Spain    \n",
       "4    from North Coast, California    \n",
       "\n",
       "                                         description  \n",
       "0   The color of this wine is deep purple with ar...  \n",
       "1   The perfect wine for the holidays, this wine ...  \n",
       "2   The wine is very approachable and fruit-forwa...  \n",
       "3   The grapes for this wine come from a selectio...  \n",
       "4   Sauvignon Blanc grapes from the North Coast v...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_df = pd.DataFrame.from_dict(generated_descriptions.items())\n",
    "wine_df.columns = ['name', 'response']\n",
    "print(wine_df.shape)\n",
    "\n",
    "wine_df['category_1'] = wine_df['response'].str.split('\\[category_1\\]').str[1].str.split('\\[category_2\\]').str[0]\n",
    "wine_df['category_2'] = wine_df['response'].str.split('\\[category_2\\]').str[1].str.split('\\[origin\\]').str[0]\n",
    "wine_df['origin'] = wine_df['response'].str.split('\\[origin\\]').str[1].str.split('\\[description\\]').str[0]\n",
    "wine_df['description'] = wine_df['response'].str.split('\\[description\\]').str[1]\n",
    "#wine_df['description'] = wine_df['description'].str.strip()\n",
    "#wine_df['description'] = wine_df['description'].str.strip('\"')\n",
    "\n",
    "#wine_df = wine_df[wine_df['description'].str.len() > 100]\n",
    "wine_df = wine_df.applymap(str)\\\n",
    "            .applymap(lambda x: x.replace('\"', ''))\\\n",
    "            .drop(['response'], axis=1)\n",
    "\n",
    "print(wine_df.shape)\n",
    "wine_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T01:29:06.100823Z",
     "start_time": "2021-01-28T01:29:06.081823Z"
    }
   },
   "outputs": [],
   "source": [
    "wine_df.to_csv(\"data/fake/descriptions/gpt2_desc_v2_20210127.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
