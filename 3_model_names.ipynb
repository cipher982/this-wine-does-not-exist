{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n",
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import six\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import os\n",
    "import datetime as dt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tensorflow import keras\n",
    "\n",
    "import random\n",
    "import string\n",
    "\n",
    "#from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "#from keras.models import load_model\n",
    "\n",
    "SCRAPED_WINES_INPUT_PATH = 'data/scraped/scraped_with_decs.pickle'\n",
    "MODEL_WEIGHTS_PATH = 'data/models_weights/name_model_weights.h5'\n",
    "\n",
    "# Set log handler\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "log = logging.getLogger(\"modelNames-logger\")\n",
    "log.setLevel(logging.INFO)\n",
    "\n",
    "# TensorFlow\n",
    "print(tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "#!pip install tensorflow-gpu==1.13.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_built_with_cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_built_with_gpu_support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "I0722 17:22:09.047432 140265745508160 <ipython-input-7-c4e72d21bf7f>:34] Loaded helper functions\n"
     ]
    }
   ],
   "source": [
    "def transform(txt, pad_to=None):\n",
    "    # drop any non-ascii characters\n",
    "    output = np.asarray([ord(c) for c in txt if ord(c) < 255], dtype=np.int32)\n",
    "    if pad_to is not None:\n",
    "        output = output[:pad_to]\n",
    "        output = np.concatenate([\n",
    "            np.zeros([pad_to - len(txt)], dtype=np.int32),\n",
    "            output\n",
    "        ])\n",
    "    return output\n",
    "\n",
    "def training_generator(seq_len=100, batch_size=1024):\n",
    "    \"\"\"A generator yields (source, target) arrays for training.\"\"\"\n",
    "    wine_data = pd.read_pickle(SCRAPED_WINES_INPUT_PATH)\n",
    "    wine_data = wine_data['name'] # Take just the names for modeling\n",
    "    txt = '\\n'.join(wine_data)\n",
    "\n",
    "    #tf.logging.info('Input text [%d] %s', len(txt), txt[:50])\n",
    "    source = transform(txt)\n",
    "    while True:\n",
    "        offsets = np.random.randint(0, len(source) - seq_len, batch_size)\n",
    "\n",
    "        # Our model uses sparse crossentropy loss, but Keras requires labels\n",
    "        # to have the same rank as the input logits.  We add an empty final\n",
    "        # dimension to account for this.\n",
    "        yield (\n",
    "            np.stack([source[idx:idx + seq_len] for idx in offsets]),\n",
    "            np.expand_dims(\n",
    "                np.stack([source[idx + 1:idx + seq_len + 1] for idx in offsets]), \n",
    "                -1),\n",
    "        )\n",
    "\n",
    "#six.next(training_generator(seq_len=10, batch_size=1))\n",
    "log.info(\"Loaded helper functions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model and train it on dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0722 17:22:25.307595 140265745508160 deprecation.py:323] From <ipython-input-8-a955dd7a7579>:51: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "seed (InputLayer)            [(1024, 100)]             0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (1024, 100, 128)          32768     \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (1024, 100, 128)          131584    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (1024, 100, 128)          131584    \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (1024, 100, 256)          33024     \n",
      "=================================================================\n",
      "Total params: 328,960\n",
      "Trainable params: 328,960\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/25\n",
      "100/100 [==============================] - ETA: 0s - loss: 3.5794 - sparse_categorical_accuracy: 0.1181\n",
      "Epoch 00001: sparse_categorical_accuracy improved from -inf to 0.11809, saving model to model_names_chkpt.h5\n",
      "100/100 [==============================] - 121s 1s/step - loss: 3.5794 - sparse_categorical_accuracy: 0.1181\n",
      "Epoch 2/25\n",
      " 14/100 [===>..........................] - ETA: 1:36 - loss: 3.4533 - sparse_categorical_accuracy: 0.1189"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0722 17:24:57.547252 140265745508160 ultratb.py:149] Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/mnt/c/Users/david/Documents/github/this-wine-does-not-exist/.venv_osx/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3291, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-8-a955dd7a7579>\", line 51, in <module>\n",
      "    verbose = 1\n",
      "  File \"/mnt/c/Users/david/Documents/github/this-wine-does-not-exist/.venv_osx/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 324, in new_func\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/mnt/c/Users/david/Documents/github/this-wine-does-not-exist/.venv_osx/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 1479, in fit_generator\n",
      "    initial_epoch=initial_epoch)\n",
      "  File \"/mnt/c/Users/david/Documents/github/this-wine-does-not-exist/.venv_osx/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 66, in _method_wrapper\n",
      "    return method(self, *args, **kwargs)\n",
      "  File \"/mnt/c/Users/david/Documents/github/this-wine-does-not-exist/.venv_osx/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 848, in fit\n",
      "    tmp_logs = train_function(iterator)\n",
      "  File \"/mnt/c/Users/david/Documents/github/this-wine-does-not-exist/.venv_osx/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\", line 580, in __call__\n",
      "    result = self._call(*args, **kwds)\n",
      "  File \"/mnt/c/Users/david/Documents/github/this-wine-does-not-exist/.venv_osx/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\", line 611, in _call\n",
      "    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\n",
      "  File \"/mnt/c/Users/david/Documents/github/this-wine-does-not-exist/.venv_osx/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 2420, in __call__\n",
      "    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\n",
      "  File \"/mnt/c/Users/david/Documents/github/this-wine-does-not-exist/.venv_osx/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 1665, in _filtered_call\n",
      "    self.captured_inputs)\n",
      "  File \"/mnt/c/Users/david/Documents/github/this-wine-does-not-exist/.venv_osx/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 1746, in _call_flat\n",
      "    ctx, args, cancellation_manager=cancellation_manager))\n",
      "  File \"/mnt/c/Users/david/Documents/github/this-wine-does-not-exist/.venv_osx/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 598, in call\n",
      "    ctx=ctx)\n",
      "  File \"/mnt/c/Users/david/Documents/github/this-wine-does-not-exist/.venv_osx/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\", line 60, in quick_execute\n",
      "    inputs, attrs, num_outputs)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/c/Users/david/Documents/github/this-wine-does-not-exist/.venv_osx/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2033, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/c/Users/david/Documents/github/this-wine-does-not-exist/.venv_osx/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1095, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/mnt/c/Users/david/Documents/github/this-wine-does-not-exist/.venv_osx/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 313, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/mnt/c/Users/david/Documents/github/this-wine-does-not-exist/.venv_osx/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 347, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 1490, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 1448, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 742, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/usr/lib/python3.6/posixpath.py\", line 395, in realpath\n",
      "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
      "  File \"/usr/lib/python3.6/posixpath.py\", line 429, in _joinrealpath\n",
      "    if not islink(newpath):\n",
      "  File \"/usr/lib/python3.6/posixpath.py\", line 171, in islink\n",
      "    st = os.lstat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "#import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "EMBEDDING_DIM = 128\n",
    "EPOCHS = 25\n",
    "\n",
    "def lstm_model(seq_len=100, batch_size=None, stateful=True):\n",
    "    \"\"\"Language model: predict the next word given the current word.\"\"\"\n",
    "    source = tf.keras.Input(\n",
    "        name='seed', shape=(seq_len,), batch_size=batch_size, dtype=tf.int32)\n",
    "\n",
    "    embedding = tf.keras.layers.Embedding(input_dim=256, output_dim=EMBEDDING_DIM)(source)\n",
    "    lstm_1 = tf.keras.layers.LSTM(EMBEDDING_DIM, stateful=stateful, return_sequences=True)(embedding)\n",
    "    lstm_2 = tf.keras.layers.LSTM(EMBEDDING_DIM, stateful=stateful, return_sequences=True)(lstm_1)\n",
    "    #drop_1 = tf.keras.layers.Dropout(0.2)\n",
    "    predicted_char = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(256, activation='softmax'))(lstm_2)\n",
    "    model = tf.keras.Model(inputs=[source], outputs=[predicted_char])\n",
    "    model.compile(\n",
    "        optimizer='rmsprop',\n",
    "        #optimizer=tf.keras.optimizers.RMSprop(lr=0.01),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['sparse_categorical_accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "if 'session' in locals() and session is not None:\n",
    "    print('Close interactive session')\n",
    "    session.close()\n",
    "\n",
    "training_model = lstm_model(seq_len=100, batch_size=1024, stateful=False)\n",
    "#training_model.load_weights('model_small_chkpt.h5', by_name=True)\n",
    "\n",
    "checkpoint = keras.callbacks.ModelCheckpoint('model_names_chkpt.h5', \n",
    "                             monitor='sparse_categorical_accuracy', \n",
    "                             verbose=1, \n",
    "                             save_best_only=True, \n",
    "                             mode='max')\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor='sparse_categorical_accuracy',\n",
    "                               patience=3,\n",
    "                               mode='max')\n",
    "callbacks_list = [checkpoint,early_stopping]\n",
    "\n",
    "print(training_model.summary())\n",
    "\n",
    "training_model.fit_generator(\n",
    "    training_generator(seq_len=100, batch_size=1024),\n",
    "    steps_per_epoch=100,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks = callbacks_list,\n",
    "    verbose = 1\n",
    "    )\n",
    "\n",
    "training_model.save_weights(MODEL_WEIGHTS_PATH, overwrite=True)\n",
    "\n",
    "# 16 = 3.5 loss (2 epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show sample of created wine names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTION 0\n",
      "\n",
      "\n",
      "7ron Sauvignon Blanc 2014\n",
      "CoxchaKniw Oilelta Drapoyo 2017\n",
      "Vina Chert Henrled Vintner Pardr Pinot Noir 2001\n",
      "Harmwake Phite Chardonnay 2012\n",
      "Alte Dare Ca Rito Viogu 2017\n",
      "Blakig Mounton Vina Grise (Futures du Grand Estate 2014\n",
      "Jamone Maruso Villagi d'Arb\n",
      "\n",
      "PREDICTION 1\n",
      "\n",
      "\n",
      "7is Carrair Cabernet Sauvignon 2014\n",
      "Canter Cabernet Sauvignon 2012\n",
      "Pewis Gramvan Solan Zincs Malbec 1994\n",
      "Tlat Mayon Cabern Chardonnay 2013\n",
      "M. Chapa Pinot Noir 2008\n",
      "Montarra Farls Cote Treviers Stermiin Vineyard Pinot Noir 2015\n",
      "Tormor Mise Sauvignon B\n",
      "\n",
      "PREDICTION 2\n",
      "\n",
      "\n",
      "7rifors)\n",
      "ReOd Estre Red d'Or Moutir de Vauventer Vineyard Ext Belle Vineyard Pinot Noir 2014\n",
      "Terred Moncanay 2004\n",
      "Nichiol Brheuth Cabernet Sauvignon 1998\n",
      "Barnasco Muciner Bruz Zinfandel 2000\n",
      "Holby Alepusco de Hervay Cabernet Sauvignon 1998\n",
      "Quarta Ric\n",
      "\n",
      "PREDICTION 3\n",
      "\n",
      "\n",
      "7ilt Zinfandel 2001\n",
      "Klain Cabernet Shiraz 2001\n",
      "Ville Heymeffiel Ricia 2014\n",
      "Domaine Prematt Liomaine Vineyard Fares Vineyard Pinot Noir 2018\n",
      "Dan-Am. Charbynon 2012\n",
      "Irnois Orerin Cote\n",
      "Layeu Larekeh La-Ciciulliaro Lete 2017\n",
      "G.SLuil & Draik Ruesivard Cab\n",
      "\n",
      "PREDICTION 4\n",
      "\n",
      "\n",
      "7herer 2010\n",
      "Rofh Calsffond Shandon Bordari Sin Giak Cabernet Sauvignon 2000\n",
      "Muchiver Enro Camima' Willatt Cabernet Sauvignon 1999\n",
      "Noda Oah Vaillap Estate Riesling Sauvignon Blanc ( 20013 Irrechini Pinot Noir 2012\n",
      "Bed Sarquas Creak Vinetard Cabernet S\n",
      "\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 5\n",
    "PREDICT_LEN = 250\n",
    "\n",
    "# Keras requires the batch size be specified ahead of time for stateful models.\n",
    "# We use a sequence length of 1, as we will be feeding in one character at a \n",
    "# time and predicting the next character.\n",
    "prediction_model = lstm_model(seq_len=1, batch_size=BATCH_SIZE, stateful=True)\n",
    "prediction_model.load_weights(MODEL_WEIGHTS_PATH)\n",
    "\n",
    "seed_txt = ''.join(random.choices(string.ascii_uppercase + string.digits, k=20))\n",
    "seed = transform(seed_txt)\n",
    "seed = np.repeat(np.expand_dims(seed, 0), BATCH_SIZE, axis=0)\n",
    "\n",
    "# First, run the seed forward to prime the state of the model.\n",
    "prediction_model.reset_states()\n",
    "for i in range(len(seed_txt) - 1):\n",
    "    prediction_model.predict(seed[:, i:i + 1])\n",
    "\n",
    "# Now we can accumulate predictions!\n",
    "predictions = [seed[:, -1:]]\n",
    "for i in range(PREDICT_LEN):\n",
    "    last_word = predictions[-1]\n",
    "    next_probits = prediction_model.predict(last_word)[:, 0, :]\n",
    "  \n",
    "  # sample from our output distribution\n",
    "    next_idx = [\n",
    "        np.random.choice(256, p=next_probits[i])\n",
    "        for i in range(BATCH_SIZE)\n",
    "    ]\n",
    "    predictions.append(np.asarray(next_idx, dtype=np.int32))\n",
    "    \n",
    "for i in range(BATCH_SIZE):\n",
    "    print('PREDICTION %d\\n\\n' % i)\n",
    "    p = [predictions[j][i] for j in range(PREDICT_LEN)]\n",
    "    generated = ''.join([chr(c) for c in p])\n",
    "    print(generated)\n",
    "    print()\n",
    "    assert len(generated) == PREDICT_LEN, 'Generated text too short'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create larger fake wine name list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [36:32<00:00,  2.19s/it]\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 5\n",
    "PREDICT_LEN = 150\n",
    "TOTAL_BATCHES = 1000\n",
    "\n",
    "# We use a sequence length of 1, as we will be feeding in one character at a \n",
    "# time and predicting the next character.\n",
    "prediction_model = lstm_model(seq_len=1, batch_size=BATCH_SIZE, stateful=True)\n",
    "prediction_model.load_weights(MODEL_WEIGHTS_PATH)\n",
    "\n",
    "fake_names = []\n",
    "for ii in tqdm(range(TOTAL_BATCHES)):\n",
    "    seed_txt = ''.join(random.choices(string.ascii_uppercase + string.digits, k=20))\n",
    "    seed = transform(seed_txt)\n",
    "    seed = np.repeat(np.expand_dims(seed, 0), BATCH_SIZE, axis=0)\n",
    "\n",
    "    # First, run the seed forward to prime the state of the model.\n",
    "    prediction_model.reset_states()\n",
    "    for i in range(len(seed_txt) - 1):\n",
    "        prediction_model.predict(seed[:, i:i + 1])\n",
    "\n",
    "    # Now we can accumulate predictions!\n",
    "    predictions = [seed[:, -1:]]\n",
    "    for i in range(PREDICT_LEN):\n",
    "        last_word = predictions[-1]\n",
    "        next_probits = prediction_model.predict(last_word)[:, 0, :]\n",
    "\n",
    "      # sample from our output distribution\n",
    "        next_idx = [\n",
    "            np.random.choice(256, p=next_probits[i])\n",
    "            for i in range(BATCH_SIZE)\n",
    "        ]\n",
    "        predictions.append(np.asarray(next_idx, dtype=np.int32))\n",
    "\n",
    "    for i in range(BATCH_SIZE):\n",
    "        #print('PREDICTION %d\\n\\n' % i)\n",
    "        p = [predictions[j][i] for j in range(PREDICT_LEN)]\n",
    "        generated = ''.join([chr(c) for c in p])\n",
    "        gen_list = generated.split('\\n')[1:-1]\n",
    "        for item in gen_list:\n",
    "            fake_names.append(item)\n",
    "        assert len(generated) == PREDICT_LEN, 'Generated text too short'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved fake_names_13301_2020-05-20.pickle\n"
     ]
    }
   ],
   "source": [
    "fileName = \"fake_names_{}_{}.pickle\".format(len(fake_names),str(dt.date.today()))\n",
    "pd.to_pickle(fake_names, 'data/fake/' + fileName)\n",
    "print(\"Saved {}\".format(fileName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving 13,301 fake names to data/fake/fake_names_13301_2020-05-20.csv\n"
     ]
    }
   ],
   "source": [
    "file_path = \"data/fake/\"\n",
    "file_name = \"fake_names_{}_{}.csv\".format(len(fake_names),str(dt.date.today()))\n",
    "print(f\"Saving {len(fake_names):,} fake names to {file_path + file_name}\")\n",
    "pd.Series(fake_names).to_csv(f\"{file_path + file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute fake wine name similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## This code is super slow if matching large lists ##########\n",
    "\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "def similar(a, b):\n",
    "    return SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "#fake_names = test['name']\n",
    "real_names = pd.read_pickle(SCRAPED_WINES_INPUT_PATH)['name']\n",
    "\n",
    "fake_scores = {}\n",
    "for f_name in tqdm(fake_names):\n",
    "    max_score = 0.0\n",
    "    for r_name in real_names:\n",
    "        score = similar(f_name, r_name)\n",
    "        if score > max_score:\n",
    "            max_score = score\n",
    "    fake_scores[f_name] = max_score\n",
    "        \n",
    "    \n",
    "fake_scores = pd.Series(fake_scores)\n",
    "\n",
    "########## This code is super slow if matching large lists ##########"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ^^ Above code takes ~8 hours to run, the files in the extra_code directory will split it out among 8 processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for wine names that match real ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_scores = pd.Series(pd.read_pickle('new_scores.pickle')[0])\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('default')\n",
    "print(\"Total fake names: \",len(fake_scores))\n",
    "print(\"Total 90% or less match: \",len(fake_scores[fake_scores < 0.9]))\n",
    "\n",
    "plt.figure(dpi=100)\n",
    "plt.title('Disitribution of fake wine names to real ones')\n",
    "plt.hist(fake_scores.values)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
