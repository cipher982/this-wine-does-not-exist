{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import six\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import random\n",
    "import string\n",
    "\n",
    "import gpt_2_simple as gpt2\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\";  \n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "EMBEDDING_DIM = 512\n",
    "SCRAPED_DATA_PATH = 'data/scraped/descriptions.pickle'\n",
    "FAKE_NAMES_PATH = 'data/fake/fake_names_12949.pickle'\n",
    "MODEL_WEIGHTS_PATH = 'data/models_weights/model_description_weights.h5'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\david\\documents\\github\\this-wine-does-not-exist\\.venv_w10\\lib\\site-packages\\ipykernel_launcher.py:2: FutureWarning: The signature of `Series.to_csv` was aligned to that of `DataFrame.to_csv`, and argument 'header' will change its default value from False to True: please pass an explicit value to suppress this warning.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "names_raw, descs_raw = pd.read_pickle(SCRAPED_DATA_PATH)\n",
    "pd.Series(descs_raw).to_csv(\"data/scraped/descriptions.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint models\\117M\\model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from models\\117M\\model.ckpt\n",
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                       | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x9d in position 7972: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-2aa4b0e8c009>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0msess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgpt2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart_tf_sess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mgpt2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfinetune\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'data/scraped/descriptions.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m# steps is max number of training steps\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#gpt2.generate(sess)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\david\\documents\\github\\this-wine-does-not-exist\\.venv_w10\\lib\\site-packages\\gpt_2_simple\\gpt_2.py\u001b[0m in \u001b[0;36mfinetune\u001b[1;34m(sess, dataset, steps, model_name, combine, batch_size, learning_rate, accumulate_gradients, restore_from, run_name, sample_every, sample_length, sample_num, save_every, print_every, max_checkpoints, model_load)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Loading dataset...'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 164\u001b[1;33m     \u001b[0mchunks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcombine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    165\u001b[0m     \u001b[0mdata_sampler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSampler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'dataset has'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_sampler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'tokens'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\david\\documents\\github\\this-wine-does-not-exist\\.venv_w10\\lib\\site-packages\\gpt_2_simple\\src\\load_dataset.py\u001b[0m in \u001b[0;36mload_dataset\u001b[1;34m(enc, path, combine)\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[1;31m# Plain text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m                 \u001b[0mraw_text\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_text\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mcombine\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m                 \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\david\\documents\\github\\this-wine-does-not-exist\\.venv_w10\\lib\\encodings\\cp1252.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mStreamWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCodec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStreamWriter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x9d in position 7972: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "#gpt2.download_gpt2()   # model is saved into current directory under /models/117M/\n",
    "\n",
    "sess = gpt2.start_tf_sess()\n",
    "gpt2.finetune(sess, 'data/scraped/descriptions.txt', steps=1000)   # steps is max number of training steps\n",
    "\n",
    "#gpt2.generate(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text to arrays of letters represented as integers\n",
    "def transform(txt, pad_to=None):\n",
    "    # drop any non-ascii characters\n",
    "    output = np.asarray([ord(c) for c in txt if ord(c) < 255], dtype=np.int32)\n",
    "    if pad_to is not None:\n",
    "        output = output[:pad_to]\n",
    "        output = np.concatenate([\n",
    "            np.zeros([pad_to - len(txt)], dtype=np.int32),\n",
    "            output\n",
    "        ])\n",
    "    return output\n",
    "\n",
    "# How the characters will be fed into the model\n",
    "def training_generator(seq_len=100, batch_size=1024):\n",
    "    \"\"\"A generator yields (source, target) arrays for training.\"\"\"\n",
    "    names_raw, descs_raw = pd.read_pickle(SCRAPED_DATA_PATH)\n",
    "    txt = '\\n'.join(descs_raw)\n",
    "    tf.logging.info('Input text [%d] %s', len(txt), txt[:50])\n",
    "    source = transform(txt)\n",
    "    while True:\n",
    "        offsets = np.random.randint(0, len(source) - seq_len, batch_size)\n",
    "        yield (\n",
    "            np.stack([source[idx:idx + seq_len] for idx in offsets]),\n",
    "            np.expand_dims(\n",
    "                np.stack([source[idx + 1:idx + seq_len + 1] for idx in offsets]), \n",
    "                -1),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "INFO:tensorflow:Input text [4733481] Dark garnet in color, the 2013 HALL Napa Valley Ca\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 4.4244 - sparse_categorical_accuracy: 0.1398\n",
      "Epoch 00001: sparse_categorical_accuracy improved from -inf to 0.13997, saving model to data/models_weights/model_char_DESCS_chkpt.h5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "100/100 [==============================] - 167s 2s/step - loss: 4.4139 - sparse_categorical_accuracy: 0.1400\n",
      "Epoch 2/50\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 3.1417 - sparse_categorical_accuracy: 0.1566\n",
      "Epoch 00002: sparse_categorical_accuracy improved from 0.13997 to 0.15756, saving model to data/models_weights/model_char_DESCS_chkpt.h5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "100/100 [==============================] - 165s 2s/step - loss: 3.1363 - sparse_categorical_accuracy: 0.1576\n",
      "Epoch 3/50\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 1.8880 - sparse_categorical_accuracy: 0.4507\n",
      "Epoch 00003: sparse_categorical_accuracy improved from 0.15756 to 0.45237, saving model to data/models_weights/model_char_DESCS_chkpt.h5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "100/100 [==============================] - 164s 2s/step - loss: 1.8822 - sparse_categorical_accuracy: 0.4524\n",
      "Epoch 4/50\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 1.1456 - sparse_categorical_accuracy: 0.6557\n",
      "Epoch 00004: sparse_categorical_accuracy improved from 0.45237 to 0.65604, saving model to data/models_weights/model_char_DESCS_chkpt.h5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "100/100 [==============================] - 164s 2s/step - loss: 1.1445 - sparse_categorical_accuracy: 0.6560\n",
      "Epoch 5/50\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 0.9631 - sparse_categorical_accuracy: 0.7086\n",
      "Epoch 00005: sparse_categorical_accuracy improved from 0.65604 to 0.70876, saving model to data/models_weights/model_char_DESCS_chkpt.h5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "100/100 [==============================] - 165s 2s/step - loss: 0.9625 - sparse_categorical_accuracy: 0.7088\n",
      "Epoch 6/50\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 0.8719 - sparse_categorical_accuracy: 0.7361\n",
      "Epoch 00006: sparse_categorical_accuracy improved from 0.70876 to 0.73620, saving model to data/models_weights/model_char_DESCS_chkpt.h5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "100/100 [==============================] - 173s 2s/step - loss: 0.8715 - sparse_categorical_accuracy: 0.7362\n",
      "Epoch 7/50\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 0.8174 - sparse_categorical_accuracy: 0.7520\n",
      "Epoch 00007: sparse_categorical_accuracy improved from 0.73620 to 0.75199, saving model to data/models_weights/model_char_DESCS_chkpt.h5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "100/100 [==============================] - 176s 2s/step - loss: 0.8173 - sparse_categorical_accuracy: 0.7520\n",
      "Epoch 8/50\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 0.7862 - sparse_categorical_accuracy: 0.7611\n",
      "Epoch 00008: sparse_categorical_accuracy improved from 0.75199 to 0.76109, saving model to data/models_weights/model_char_DESCS_chkpt.h5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "100/100 [==============================] - 176s 2s/step - loss: 0.7863 - sparse_categorical_accuracy: 0.7611\n",
      "Epoch 9/50\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 0.7663 - sparse_categorical_accuracy: 0.7666\n",
      "Epoch 00009: sparse_categorical_accuracy improved from 0.76109 to 0.76668, saving model to data/models_weights/model_char_DESCS_chkpt.h5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "100/100 [==============================] - 177s 2s/step - loss: 0.7661 - sparse_categorical_accuracy: 0.7667\n",
      "Epoch 10/50\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 0.7548 - sparse_categorical_accuracy: 0.7697\n",
      "Epoch 00010: sparse_categorical_accuracy improved from 0.76668 to 0.76974, saving model to data/models_weights/model_char_DESCS_chkpt.h5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "100/100 [==============================] - 177s 2s/step - loss: 0.7548 - sparse_categorical_accuracy: 0.7697\n",
      "Epoch 11/50\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 0.7466 - sparse_categorical_accuracy: 0.7718\n",
      "Epoch 00011: sparse_categorical_accuracy improved from 0.76974 to 0.77182, saving model to data/models_weights/model_char_DESCS_chkpt.h5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "100/100 [==============================] - 179s 2s/step - loss: 0.7465 - sparse_categorical_accuracy: 0.7718\n",
      "Epoch 12/50\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 0.7442 - sparse_categorical_accuracy: 0.7722\n",
      "Epoch 00012: sparse_categorical_accuracy improved from 0.77182 to 0.77216, saving model to data/models_weights/model_char_DESCS_chkpt.h5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "100/100 [==============================] - 175s 2s/step - loss: 0.7443 - sparse_categorical_accuracy: 0.7722\n",
      "Epoch 13/50\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 0.7424 - sparse_categorical_accuracy: 0.7727\n",
      "Epoch 00013: sparse_categorical_accuracy improved from 0.77216 to 0.77273, saving model to data/models_weights/model_char_DESCS_chkpt.h5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "100/100 [==============================] - 166s 2s/step - loss: 0.7424 - sparse_categorical_accuracy: 0.7727\n",
      "Epoch 14/50\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 0.7422 - sparse_categorical_accuracy: 0.7726\n",
      "Epoch 00014: sparse_categorical_accuracy did not improve from 0.77273\n",
      "100/100 [==============================] - 165s 2s/step - loss: 0.7422 - sparse_categorical_accuracy: 0.7726\n",
      "Epoch 15/50\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 0.7427 - sparse_categorical_accuracy: 0.7725\n",
      "Epoch 00015: sparse_categorical_accuracy did not improve from 0.77273\n",
      "100/100 [==============================] - 164s 2s/step - loss: 0.7428 - sparse_categorical_accuracy: 0.7724\n",
      "Epoch 16/50\n",
      " 99/100 [============================>.] - ETA: 1s - loss: 0.7445 - sparse_categorical_accuracy: 0.7717\n",
      "Epoch 00016: sparse_categorical_accuracy did not improve from 0.77273\n",
      "100/100 [==============================] - 164s 2s/step - loss: 0.7446 - sparse_categorical_accuracy: 0.7716\n"
     ]
    }
   ],
   "source": [
    "def lstm_model(seq_len=100, batch_size=None, stateful=True):\n",
    "    \"\"\"Language model: predict the next word given the current word.\"\"\"\n",
    "    source = tf.keras.Input(\n",
    "        name='seed', shape=(seq_len,), batch_size=batch_size, dtype=tf.int32)\n",
    "\n",
    "    embedding = tf.keras.layers.Embedding(input_dim=256, output_dim=EMBEDDING_DIM)(source)\n",
    "    lstm_1 = tf.keras.layers.LSTM(EMBEDDING_DIM, stateful=stateful, return_sequences=True)(embedding)\n",
    "    lstm_2 = tf.keras.layers.LSTM(EMBEDDING_DIM, stateful=stateful, return_sequences=True)(lstm_1)\n",
    "    #drop_1 = tf.keras.layers.Dropout(0.2)\n",
    "    predicted_char = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(256, activation='softmax'))(lstm_2)\n",
    "    model = tf.keras.Model(inputs=[source], outputs=[predicted_char])\n",
    "    #model = tf.keras.utils.multi_gpu_model(model, gpus=2)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.train.RMSPropOptimizer(learning_rate=0.01),\n",
    "        #optimizer=tf.keras.optimizers.RMSprop(lr=0.01),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['sparse_categorical_accuracy'])\n",
    "    return model\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "training_model = lstm_model(seq_len=100, batch_size=1024, stateful=False)\n",
    "#training_model.load_weights('model_small_chkpt.h5', by_name=True)\n",
    "\n",
    "checkpoint = ModelCheckpoint('data/models_weights/model_char_DESCS_chkpt.h5', \n",
    "                             monitor='sparse_categorical_accuracy', \n",
    "                             verbose=1, \n",
    "                             save_best_only=True, \n",
    "                             mode='max')\n",
    "early_stopping = EarlyStopping(monitor='sparse_categorical_accuracy',\n",
    "                               patience=5,\n",
    "                               mode='max')\n",
    "callbacks_list = [checkpoint,early_stopping]\n",
    "\n",
    "\n",
    "\n",
    "training_model.fit_generator(\n",
    "    training_generator(seq_len=100, batch_size=1024),\n",
    "    steps_per_epoch=100,\n",
    "    epochs=50,\n",
    "    callbacks = callbacks_list\n",
    "    )\n",
    "\n",
    "training_model.save_weights(MODEL_WEIGHTS_PATH, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "seed (InputLayer)            (1024, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (1024, 100, 512)          131072    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (1024, 100, 512)          2099200   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (1024, 100, 512)          2099200   \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (1024, 100, 256)          131328    \n",
      "=================================================================\n",
      "Total params: 4,460,800\n",
      "Trainable params: 4,460,800\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "training_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show sample of created wine descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTION 0\n",
      "\n",
      "\n",
      "BFres Pea or California.\n",
      "Who Franci Megrey aimon facility that's from which Mrisas prosish, this wine is concentrated sharp and sweetness.\n",
      "Deep and subtle mix of figs and fresh pith. A spicy peach with spicy peach, pineapple and passion fruit. The palate is rich and aromatized and texture. The nose presents aromas of red cherry and plums or finesse\n",
      "\n",
      "PREDICTION 1\n",
      "\n",
      "\n",
      "BA Right Crosselt's wine enthusiasts in Magdaleto, original premier cru spent to grow, Chardonnay and Zinfandel in Sonomas Russian River Valley and to retain from the concrete. it is a rare of steely climate fully better sits gentle date between lofts to drink.\n",
      "Since 1880, Maison Joseph Drouhin heritang year hold yees long toward a number of vineya\n",
      "\n",
      "PREDICTION 2\n",
      "\n",
      "\n",
      "B'l Latour years ago, it would be a great place to calca however the flavor of the Herriard one of the most varieties, including Marsanne's displays more vanilla bean and citrus notes, hints of fresh spice and roasted hazelnut. \n",
      "\n",
      "\n",
      "Le Secret is the ideal glass. The 2017 Gualtall Lake Reach, the property and the Etnange is often described as \"an iron\n",
      "\n",
      "PREDICTION 3\n",
      "\n",
      "\n",
      "BRobert, a Pinot Noir has an introduction of Pantrea Rhone is very still offering an incredible arrived in stainless steel strength and a relense, regardle in the Monarch of Andre Tcheric Sonoma Coast appellation on Zinfandel vineyard (SLarrio), the Rocche has chosen to majestic expand and right superb flavor. Serve its spicy raspberry and cherry l\n",
      "\n",
      "PREDICTION 4\n",
      "\n",
      "\n",
      "Bin perfectly raspberry, forest floral, fuji and fresh, bread snappel tint notes and hints of white gold. Aromas of passion fruit, lemongrass, lime zest, lime, fig and pepper. Droaming sparkling wine.  A rich bouquet of dark fruits combine with hints of red berries and cream-background of austere Winegrowers, but as a grower pit-kilien cranberry-th\n",
      "\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 5\n",
    "PREDICT_LEN = 350\n",
    "EMBEDDING_DIM = 512\n",
    "MODEL_WEIGHTS_PATH = 'data/models_weights/model_description_weights.h5'\n",
    "\n",
    "# Keras requires the batch size be specified ahead of time for stateful models.\n",
    "# We use a sequence length of 1, as we will be feeding in one character at a \n",
    "# time and predicting the next character.\n",
    "prediction_model = lstm_model(seq_len=1, batch_size=BATCH_SIZE, stateful=True)\n",
    "prediction_model.load_weights(MODEL_WEIGHTS_PATH)\n",
    "\n",
    "# We seed the model with our initial string, copied BATCH_SIZE times\n",
    "seed_txt = 'This wine tastes like '\n",
    "seed_txt = ''.join(random.choices(string.ascii_uppercase + string.digits, k=20))\n",
    "seed = transform(seed_txt)\n",
    "seed = np.repeat(np.expand_dims(seed, 0), BATCH_SIZE, axis=0)\n",
    "\n",
    "# First, run the seed forward to prime the state of the model.\n",
    "prediction_model.reset_states()\n",
    "for i in range(len(seed_txt) - 1):\n",
    "    prediction_model.predict(seed[:, i:i + 1])\n",
    "\n",
    "# Now we can accumulate predictions!\n",
    "predictions = [seed[:, -1:]]\n",
    "for i in range(PREDICT_LEN):\n",
    "    last_word = predictions[-1]\n",
    "    next_probits = prediction_model.predict(last_word)[:, 0, :]\n",
    "  \n",
    "  # sample from our output distribution\n",
    "    next_idx = [\n",
    "        np.random.choice(256, p=next_probits[i])\n",
    "        for i in range(BATCH_SIZE)\n",
    "    ]\n",
    "    predictions.append(np.asarray(next_idx, dtype=np.int32))\n",
    "    \n",
    "for i in range(BATCH_SIZE):\n",
    "    print('PREDICTION %d\\n\\n' % i)\n",
    "    p = [predictions[j][i] for j in range(PREDICT_LEN)]\n",
    "    generated = ''.join([chr(c) for c in p])\n",
    "    print(generated)\n",
    "    print()\n",
    "    assert len(generated) == PREDICT_LEN, 'Generated text too short'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create larger fake wine description list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 1275/1275 [35:03<00:00,  1.63s/it]\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 1\n",
    "PREDICT_LEN = 600\n",
    "N_PREDICTIONS = 100\n",
    "EMBEDDING_DIM = 512\n",
    "MODEL_WEIGHTS_PATH = 'data/models_weights/model_description_weights.h5'\n",
    "\n",
    "\n",
    "# Keras requires the batch size be specified ahead of time for stateful models.\n",
    "# We use a sequence length of 1, as we will be feeding in one character at a \n",
    "# time and predicting the next character.\n",
    "prediction_model = lstm_model(seq_len=1, batch_size=BATCH_SIZE, stateful=True)\n",
    "prediction_model.load_weights(MODEL_WEIGHTS_PATH)\n",
    "predicted_names = pd.read_csv('data/fake/NAMES_v1.csv')\n",
    "\n",
    "N_PREDICTIONS = len(predicted_names)\n",
    "\n",
    "fake_NAME = []\n",
    "fake_DESC = []\n",
    "for ii in tqdm(range(N_PREDICTIONS)):\n",
    "    # We seed the model with our initial string, copied BATCH_SIZE times\n",
    "    #seed_array = np.zeros(shape=(BATCH_SIZE,))\n",
    "    for i in range(BATCH_SIZE):\n",
    "        seed_txt = predicted_names['name'][ii+i]\n",
    "        seed = transform(seed_txt)\n",
    "        #print(seed.shape)\n",
    "    seed = np.repeat(np.expand_dims(seed, 0), BATCH_SIZE, axis=0)\n",
    "\n",
    "    # First, run the seed forward to prime the state of the model.\n",
    "    prediction_model.reset_states()\n",
    "    for i in range(len(seed_txt) - 1):\n",
    "        prediction_model.predict(seed[:, i:i + 1])\n",
    "\n",
    "    # Now we can accumulate predictions!\n",
    "    predictions = [seed[:, -1:]]\n",
    "    for i in range(PREDICT_LEN):\n",
    "        last_word = predictions[-1]\n",
    "        next_probits = prediction_model.predict(last_word)[:, 0, :]\n",
    "\n",
    "      # sample from our output distribution\n",
    "        next_idx = [\n",
    "            np.random.choice(256, p=next_probits[i])\n",
    "            for i in range(BATCH_SIZE)\n",
    "        ]\n",
    "        predictions.append(np.asarray(next_idx, dtype=np.int32))\n",
    "\n",
    "    for i in range(BATCH_SIZE):\n",
    "        #print('PREDICTION %d\\n\\n' % i)\n",
    "        p = [predictions[j][i] for j in range(PREDICT_LEN)]\n",
    "        generated = ''.join([chr(c) for c in p])\n",
    "        #print(generated)\n",
    "        #print()\n",
    "        gen_list = generated.split('.')[1:-1]\n",
    "        gen_conc = ' '.join(gen_list) + '.'\n",
    "        fake_NAME.append(seed_txt)\n",
    "        fake_DESC.append(gen_conc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'name'        : fake_NAME,\n",
    "              'description' : fake_DESC})\\\n",
    "    .to_excel('data/fake/names_descriptions.xlsx', index=False,\n",
    "             engine='xlsxwriter')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
