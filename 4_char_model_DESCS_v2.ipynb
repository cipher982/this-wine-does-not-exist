{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import six\n",
    "import keras as k \n",
    "import time\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import simplejson\n",
    "\n",
    "import random\n",
    "import string\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import load_model, model_from_json\n",
    "\n",
    "#print(k.backend.tensorflow_backend._get_available_gpus())\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "%matplotlib inline\n",
    "print(\"Loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text [%d] %s 4733481 Dark garnet in color, the 2013 HALL Napa Valley Ca\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[110,  97, 108,  32,  98, 114,  97, 110, 100,  32]]), array([[[ 97],\n",
       "         [108],\n",
       "         [ 32],\n",
       "         [ 98],\n",
       "         [114],\n",
       "         [ 97],\n",
       "         [110],\n",
       "         [100],\n",
       "         [ 32],\n",
       "         [116]]]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_PATH = 'data/pickles/descriptions.pickle'\n",
    "\n",
    "#tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "def transform(txt, pad_to=None):\n",
    "    # drop any non-ascii characters\n",
    "    output = np.asarray([ord(c) for c in txt if ord(c) < 255], dtype=np.int32)\n",
    "    if pad_to is not None:\n",
    "        output = output[:pad_to]\n",
    "        output = np.concatenate([\n",
    "            np.zeros([pad_to - len(txt)], dtype=np.int32),\n",
    "            output\n",
    "        ])\n",
    "    return output\n",
    "\n",
    "def training_generator(seq_len=100, batch_size=1024):\n",
    "    \"\"\"A generator yields (source, target) arrays for training.\"\"\"\n",
    "    names_raw, descs_raw = pd.read_pickle(DATA_PATH)\n",
    "    txt = '\\n'.join(descs_raw)\n",
    "\n",
    "    print('Input text [%d] %s', len(txt), txt[:50])\n",
    "    source = transform(txt)\n",
    "    while True:\n",
    "        offsets = np.random.randint(0, len(source) - seq_len, batch_size)\n",
    "\n",
    "        # Our model uses sparse crossentropy loss, but Keras requires labels\n",
    "        # to have the same rank as the input logits.  We add an empty final\n",
    "        # dimension to account for this.\n",
    "        yield (\n",
    "            np.stack([source[idx:idx + seq_len] for idx in offsets]),\n",
    "            np.expand_dims(\n",
    "                np.stack([source[idx + 1:idx + seq_len + 1] for idx in offsets]), \n",
    "                -1),\n",
    "        )\n",
    "\n",
    "six.next(training_generator(seq_len=10, batch_size=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n",
      "200\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (1024, 200)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (1024, 200, 512)          131072    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (1024, 200, 512)          2099200   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (1024, 200, 512)          2099200   \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (1024, 200, 256)          131328    \n",
      "=================================================================\n",
      "Total params: 4,460,800\n",
      "Trainable params: 4,460,800\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 512\n",
    "from keras.layers import LSTM, Input, Embedding\n",
    "from keras.models import Sequential\n",
    "import keras as k\n",
    "import tensorflow as tf\n",
    "\n",
    "def lstm_model(seq_len=200, batch_size=None, stateful=True):\n",
    "    print(batch_size)\n",
    "    print(seq_len)\n",
    "    \"\"\"Language model: predict the next word given the current word.\"\"\"\n",
    "    source = k.Input(batch_shape=(batch_size, seq_len), dtype=tf.int32)\n",
    "    embedding = k.layers.Embedding(input_dim=256, output_dim=EMBEDDING_DIM)(source)\n",
    "    lstm_1 = k.layers.LSTM(EMBEDDING_DIM, stateful=stateful, return_sequences=True)(embedding)\n",
    "    lstm_2 = k.layers.LSTM(EMBEDDING_DIM, stateful=stateful, return_sequences=True)(lstm_1)\n",
    "    #drop_1 = tf.keras.layers.Dropout(0.2)\n",
    "    predicted_char = k.layers.TimeDistributed(k.layers.Dense(256, activation='softmax'))(lstm_2)\n",
    "    model = k.Model(inputs=[source], outputs=[predicted_char])\n",
    "    return model\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "training_model = lstm_model(seq_len=200, batch_size=1024, stateful=False)\n",
    "#with open('data/models_weights/model_char_DESCS.json', 'r') as temp:\n",
    "#    training_model = model_from_json(temp.read())\n",
    "#temp.read()\n",
    "#temp.close()\n",
    "#training_model = model_from_json(training_model)\n",
    "training_model.compile(optimizer=k.optimizers.RMSprop(lr=0.01),\n",
    "                       loss='sparse_categorical_crossentropy',\n",
    "                       metrics=['sparse_categorical_accuracy'])\n",
    "#training_model.load_weights('data/models_weights/model_char_DESCS_weights.h5')\n",
    "\n",
    "training_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "Input text [%d] %s 4733481 Dark garnet in color, the 2013 HALL Napa Valley Ca\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Incompatible shapes: [204800] vs. [1024,200]\n\t [[{{node metrics/sparse_categorical_accuracy/Equal}} = Equal[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](metrics/sparse_categorical_accuracy/Reshape, metrics/sparse_categorical_accuracy/Cast)]]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-cedbe0bd686e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcallbacks_list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     )\n\u001b[0;32m     21\u001b[0m \u001b[1;31m#training_model.save_weights('model_char_DESCS_weights_v2.h5', overwrite=True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\david\\documents\\github\\this-wine-does-not-exist\\.venv_w10\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\david\\documents\\github\\this-wine-does-not-exist\\.venv_w10\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1418\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1420\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\david\\documents\\github\\this-wine-does-not-exist\\.venv_w10\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[0;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m                                             class_weight=class_weight)\n\u001b[0m\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\david\\documents\\github\\this-wine-does-not-exist\\.venv_w10\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1217\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1218\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\david\\documents\\github\\this-wine-does-not-exist\\.venv_w10\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\david\\documents\\github\\this-wine-does-not-exist\\.venv_w10\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\david\\documents\\github\\this-wine-does-not-exist\\.venv_w10\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\david\\documents\\github\\this-wine-does-not-exist\\.venv_w10\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[0;32m    526\u001b[0m             \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    527\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 528\u001b[1;33m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[0;32m    529\u001b[0m     \u001b[1;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m     \u001b[1;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Incompatible shapes: [204800] vs. [1024,200]\n\t [[{{node metrics/sparse_categorical_accuracy/Equal}} = Equal[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](metrics/sparse_categorical_accuracy/Reshape, metrics/sparse_categorical_accuracy/Cast)]]"
     ]
    }
   ],
   "source": [
    "# serialize model to JSON\n",
    "#model_json = training_model.to_json()\n",
    "#with open(\"data/models_weights/model_char_DESCS.json\", \"w\") as json_file:\n",
    "#    json_file.write(simplejson.dumps(simplejson.loads(model_json), indent=4))\n",
    "\n",
    "# TRAIN\n",
    "checkpoint = ModelCheckpoint('model_char_DESCS_chkpt_v2.h5', \n",
    "                             monitor='sparse_categorical_accuracy', \n",
    "                             save_best_only=True, \n",
    "                             mode='max')\n",
    "early_stopping = EarlyStopping(monitor='sparse_categorical_accuracy',\n",
    "                               patience=3,\n",
    "                               mode='max')\n",
    "callbacks_list = [checkpoint,early_stopping]\n",
    "training_model.fit_generator(\n",
    "    training_generator(seq_len=200, batch_size=1024),\n",
    "    steps_per_epoch=200,\n",
    "    epochs=2,\n",
    "    callbacks = callbacks_list\n",
    "    )\n",
    "#training_model.save_weights('model_char_DESCS_weights_v2.h5', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Model' object has no attribute 'history'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-f4465127eb9e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtraining_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtraining_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sparse_categorical_accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdpi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m150\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Accuracy per Epoch\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Model' object has no attribute 'history'"
     ]
    }
   ],
   "source": [
    "# list all data in history\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "training_acc = training_model.history.history['sparse_categorical_accuracy']\n",
    "plt.figure(dpi=150)\n",
    "plt.title(\"Accuracy per Epoch\")\n",
    "plt.plot(range(len(training_acc)),  training_acc)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# serialize model to JSON\n",
    "model_json = training_model.to_json()\n",
    "with open(\"data/models_weights/model_char_DESCS.json\", \"w\") as json_file:\n",
    "    json_file.write(simplejson.dumps(simplejson.loads(model_json), indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTION 0\n",
      "\n",
      "\n",
      "V Pinots in Napa Valley, which sees an innovative and a Pinot Noir the Catena of the Jurava and in 1992, and has been the new winery is one of the aims at the establish foeser vesselton Cellars produces worldwide in Mendoza.\n",
      "A time to create Alexande\n",
      "\n",
      "PREDICTION 1\n",
      "\n",
      "\n",
      "V Tempranillo store a sense of place the first impresses on approachable as the final blend, dry-farmed autown winemakers also unclos daughter and the same vines by a chewy texture while exotic fruit and out finish that end to the sunny family estate\n",
      "\n",
      "PREDICTION 2\n",
      "\n",
      "\n",
      "V Pinot Noir grapes, this Chateauneuf-du-Pape is one of Grenache and a continue the distinct cellar to Frank Fador and a classic Paso Robles was founded in Sonoma County's Knid and Shiraz. Born in the worlds, used a four most renowned hot, the around\n",
      "\n",
      "PREDICTION 3\n",
      "\n",
      "\n",
      "V Pinot Noir shows flavor with a wide with a dusty structure and elegance.\n",
      "Popilia Cook Cellar Carignan has a breaks the estate wines, dedicated near the southern box of vineyard language out wine. Fresh, perfect with melon, and spice with a good yea\n",
      "\n",
      "PREDICTION 4\n",
      "\n",
      "\n",
      "V Smith Tregian State-Brother, a world-class wine grape supplier, amarena cheteries in Maipo Valley. Sleek, and picked steak the sunset at Petite Sirah in the winery in 2007, Inmanding and the result of exclusively a weekenty winemaker  the wine and \n",
      "\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 5\n",
    "PREDICT_LEN = 250\n",
    "\n",
    "# Keras requires the batch size be specified ahead of time for stateful models.\n",
    "# We use a sequence length of 1, as we will be feeding in one character at a \n",
    "# time and predicting the next character.\n",
    "prediction_model = lstm_model(seq_len=1, batch_size=BATCH_SIZE, stateful=True)\n",
    "prediction_model.load_weights('data/models_weights/model_char_DESCS_weights.h5')\n",
    "\n",
    "# We seed the model with our initial string, copied BATCH_SIZE times\n",
    "\n",
    "seed_txt = 'This wine tastes like '\n",
    "seed_txt = ''.join(random.choices(string.ascii_uppercase + string.digits, k=20))\n",
    "seed = transform(seed_txt)\n",
    "seed = np.repeat(np.expand_dims(seed, 0), BATCH_SIZE, axis=0)\n",
    "\n",
    "# First, run the seed forward to prime the state of the model.\n",
    "prediction_model.reset_states()\n",
    "for i in range(len(seed_txt) - 1):\n",
    "    prediction_model.predict(seed[:, i:i + 1])\n",
    "\n",
    "# Now we can accumulate predictions!\n",
    "predictions = [seed[:, -1:]]\n",
    "for i in range(PREDICT_LEN):\n",
    "    last_word = predictions[-1]\n",
    "    next_probits = prediction_model.predict(last_word)[:, 0, :]\n",
    "  \n",
    "  # sample from our output distribution\n",
    "    next_idx = [\n",
    "        np.random.choice(256, p=next_probits[i])\n",
    "        for i in range(BATCH_SIZE)\n",
    "    ]\n",
    "    predictions.append(np.asarray(next_idx, dtype=np.int32))\n",
    "    \n",
    "for i in range(BATCH_SIZE):\n",
    "    print('PREDICTION %d\\n\\n' % i)\n",
    "    p = [predictions[j][i] for j in range(PREDICT_LEN)]\n",
    "    generated = ''.join([chr(c) for c in p])\n",
    "    print(generated)\n",
    "    print()\n",
    "    assert len(generated) == PREDICT_LEN, 'Generated text too short'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 40 Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 5\n",
    "PREDICT_LEN = 150\n",
    "\n",
    "# Keras requires the batch size be specified ahead of time for stateful models.\n",
    "# We use a sequence length of 1, as we will be feeding in one character at a \n",
    "# time and predicting the next character.\n",
    "prediction_model = lstm_model(seq_len=1, batch_size=BATCH_SIZE, stateful=True)\n",
    "prediction_model.load_weights('model_weights.h5')\n",
    "\n",
    "fake_names = []\n",
    "for ii in tqdm(range(100)):\n",
    "    # We seed the model with our initial string, copied BATCH_SIZE times\n",
    "    seed_txt = 'This wine tastes like '\n",
    "    seed_txt = ''.join(random.choices(string.ascii_uppercase + string.digits, k=20))\n",
    "    seed = transform(seed_txt)\n",
    "    seed = np.repeat(np.expand_dims(seed, 0), BATCH_SIZE, axis=0)\n",
    "\n",
    "    # First, run the seed forward to prime the state of the model.\n",
    "    prediction_model.reset_states()\n",
    "    for i in range(len(seed_txt) - 1):\n",
    "        prediction_model.predict(seed[:, i:i + 1])\n",
    "\n",
    "    # Now we can accumulate predictions!\n",
    "    predictions = [seed[:, -1:]]\n",
    "    for i in range(PREDICT_LEN):\n",
    "        last_word = predictions[-1]\n",
    "        next_probits = prediction_model.predict(last_word)[:, 0, :]\n",
    "\n",
    "      # sample from our output distribution\n",
    "        next_idx = [\n",
    "            np.random.choice(256, p=next_probits[i])\n",
    "            for i in range(BATCH_SIZE)\n",
    "        ]\n",
    "        predictions.append(np.asarray(next_idx, dtype=np.int32))\n",
    "\n",
    "    for i in range(BATCH_SIZE):\n",
    "        #print('PREDICTION %d\\n\\n' % i)\n",
    "        p = [predictions[j][i] for j in range(PREDICT_LEN)]\n",
    "        generated = ''.join([chr(c) for c in p])\n",
    "        gen_list = generated.split('\\n')[1:-1]\n",
    "        for item in gen_list:\n",
    "            fake_names.append(item)\n",
    "        assert len(generated) == PREDICT_LEN, 'Generated text too short'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "\n",
    "def similar(a, b):\n",
    "    return SequenceMatcher(None, a, b).ratio()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_names, descs_raw = pd.read_pickle(DATA_PATH)\n",
    "\n",
    "fake_names_2 = []\n",
    "for f_name in fake_names:\n",
    "    max_score = 0.0\n",
    "    for r_name in real_names:\n",
    "        if similar(f_name,r_name) > max_score\n",
    "    \n",
    "len(fake_names_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 5\n",
    "PREDICT_LEN = 250\n",
    "\n",
    "# Keras requires the batch size be specified ahead of time for stateful models.\n",
    "# We use a sequence length of 1, as we will be feeding in one character at a \n",
    "# time and predicting the next character.\n",
    "prediction_model = lstm_model(seq_len=1, batch_size=BATCH_SIZE, stateful=True)\n",
    "prediction_model.load_weights('/tmp/bard.h5')\n",
    "\n",
    "# We seed the model with our initial string, copied BATCH_SIZE times\n",
    "\n",
    "seed_txt = 'Looks it not like the king?  Verily, we must go! '\n",
    "seed = transform(seed_txt)\n",
    "seed = np.repeat(np.expand_dims(seed, 0), BATCH_SIZE, axis=0)\n",
    "\n",
    "# First, run the seed forward to prime the state of the model.\n",
    "prediction_model.reset_states()\n",
    "for i in range(len(seed_txt) - 1):\n",
    "    prediction_model.predict(seed[:, i:i + 1])\n",
    "\n",
    "# Now we can accumulate predictions!\n",
    "predictions = [seed[:, -1:]]\n",
    "for i in range(PREDICT_LEN):\n",
    "    last_word = predictions[-1]\n",
    "    next_probits = prediction_model.predict(last_word)[:, 0, :]\n",
    "  \n",
    "  # sample from our output distribution\n",
    "    next_idx = [\n",
    "        np.random.choice(256, p=next_probits[i])\n",
    "        for i in range(BATCH_SIZE)\n",
    "    ]\n",
    "    predictions.append(np.asarray(next_idx, dtype=np.int32))\n",
    "    \n",
    "for i in range(BATCH_SIZE):\n",
    "    print('PREDICTION %d\\n\\n' % i)\n",
    "    p = [predictions[j][i] for j in range(PREDICT_LEN)]\n",
    "    generated = ''.join([chr(c) for c in p])\n",
    "    print(generated)\n",
    "    print()\n",
    "    assert len(generated) == PREDICT_LEN, 'Generated text too short'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
